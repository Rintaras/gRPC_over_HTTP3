#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
3æ—¥å‰ã®å®Ÿé¨“ç’°å¢ƒã¨ç¾åœ¨ã®å®Ÿé¨“ç’°å¢ƒã®é•ã„ã‚’åˆ†æã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Ÿé¨“ç’°å¢ƒã®æ”¹å–„ç‚¹ã¨æ€§èƒ½ã¸ã®å½±éŸ¿ã‚’è©•ä¾¡
"""

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import matplotlib.font_manager as fm
import matplotlib as mpl

japanese_fonts = [
    'Hiragino Sans', 'Hiragino Kaku Gothic ProN', 'Yu Gothic', 'Meiryo', 
    'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP', 
    'Source Han Sans JP', 'Noto Sans JP', 'M PLUS 1p', 'Kosugi Maru',
    'Hiragino Maru Gothic ProN', 'Yu Gothic UI', 'MS Gothic', 'MS Mincho'
]

available_fonts = [f.name for f in fm.fontManager.ttflist]
font_found = False
selected_font = None

for font in japanese_fonts:
    if font in available_fonts:
        selected_font = font
        font_found = True
        print(f"âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šæˆåŠŸ: {font}")
        break

if font_found:
    font_family = [selected_font, 'DejaVu Sans', 'Arial Unicode MS', 'sans-serif']
    plt.rcParams.update({
        'font.family': font_family,
        'font.sans-serif': font_family,
        'axes.unicode_minus': False,
        'font.size': 10,
        'axes.titlesize': 12,
        'axes.labelsize': 10,
        'xtick.labelsize': 9,
        'ytick.labelsize': 9,
        'legend.fontsize': 9
    })
    mpl.rcParams.update({
        'font.family': font_family,
        'font.sans-serif': font_family,
        'axes.unicode_minus': False
    })
else:
    fallback_fonts = ['DejaVu Sans', 'Arial Unicode MS', 'sans-serif']
    plt.rcParams.update({
        'font.family': fallback_fonts,
        'font.sans-serif': fallback_fonts,
        'axes.unicode_minus': False,
        'font.size': 10,
        'axes.titlesize': 12,
        'axes.labelsize': 10,
        'xtick.labelsize': 9,
        'ytick.labelsize': 9,
        'legend.fontsize': 9
    })
    mpl.rcParams.update({
        'font.family': fallback_fonts,
        'font.sans-serif': fallback_fonts,
        'axes.unicode_minus': False
    })
    print("âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

class EnvironmentComparisonAnalyzer:
    def __init__(self, logs_dir="logs"):
        self.logs_dir = Path(logs_dir)
        self.old_environment = {}
        self.new_environment = {}
        
    def find_old_environment(self):
        """3æ—¥å‰ã®å®Ÿé¨“ç’°å¢ƒã‚’æ¤œç´¢"""
        old_dirs = []
        for item in self.logs_dir.iterdir():
            if item.is_dir() and item.name.startswith('benchmark_20250723'):
                old_dirs.append(item)
        
        if old_dirs:
            # æœ€æ–°ã®ã‚‚ã®ã‚’é¸æŠ
            old_dirs.sort(key=lambda x: x.name, reverse=True)
            return old_dirs[0]
        return None
    
    def find_new_environment(self):
        """ç¾åœ¨ã®å®Ÿé¨“ç’°å¢ƒã‚’æ¤œç´¢"""
        new_dirs = []
        for item in self.logs_dir.iterdir():
            if item.is_dir() and item.name.startswith('japanese_benchmark_20250725'):
                new_dirs.append(item)
        
        if new_dirs:
            # æœ€æ–°ã®ã‚‚ã®ã‚’é¸æŠ
            new_dirs.sort(key=lambda x: x.name, reverse=True)
            return new_dirs[0]
        return None
    
    def parse_benchmark_params(self, params_file):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è§£æ"""
        params = {}
        if params_file.exists():
            with open(params_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if '=' in line:
                        key, value = line.strip().split('=', 1)
                        params[key] = value
        return params
    
    def parse_summary_file(self, summary_file):
        """ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ"""
        if not summary_file.exists():
            return {}
        
        with open(summary_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ä¸»è¦ãªæŒ‡æ¨™ã‚’æŠ½å‡º
        h3_advantage_match = re.search(r'HTTP/3 Advantage Cases: (\d+)/(\d+) Cases', content)
        h2_advantage_match = re.search(r'HTTP/2 Advantage Cases: (\d+)/(\d+) Cases', content)
        
        h3_advantage = int(h3_advantage_match.group(1)) if h3_advantage_match else 0
        total_cases = int(h3_advantage_match.group(2)) if h3_advantage_match else 0
        
        # å¹³å‡æ€§èƒ½å·®ã‚’æŠ½å‡º
        throughput_match = re.search(r'Average Throughput Advantage: ([+-]?\d+\.?\d*)%', content)
        latency_match = re.search(r'Average Latency Advantage: ([+-]?\d+\.?\d*)%', content)
        connection_match = re.search(r'Average Connection Time Advantage: ([+-]?\d+\.?\d*)%', content)
        
        throughput_adv = float(throughput_match.group(1)) if throughput_match else 0.0
        latency_adv = float(latency_match.group(1)) if latency_match else 0.0
        connection_adv = float(connection_match.group(1)) if connection_match else 0.0
        
        # æœ€å¤§æ€§èƒ½å·®ã‚’æŠ½å‡º
        max_throughput_match = re.search(r'Maximum Throughput Advantage: ([+-]?\d+\.?\d*)%', content)
        max_throughput = float(max_throughput_match.group(1)) if max_throughput_match else 0.0
        
        return {
            'h3_advantage_cases': h3_advantage,
            'total_cases': total_cases,
            'h3_advantage_ratio': h3_advantage / total_cases if total_cases > 0 else 0,
            'avg_throughput_advantage': throughput_adv,
            'avg_latency_advantage': latency_adv,
            'avg_connection_advantage': connection_adv,
            'max_throughput_advantage': max_throughput
        }
    
    def analyze_environment_differences(self):
        """ç’°å¢ƒã®é•ã„ã‚’åˆ†æ"""
        print("ğŸ” å®Ÿé¨“ç’°å¢ƒã®é•ã„ã‚’åˆ†æä¸­...")
        
        old_env_dir = self.find_old_environment()
        new_env_dir = self.find_new_environment()
        
        if not old_env_dir or not new_env_dir:
            print("âŒ æ¯”è¼ƒå¯èƒ½ãªç’°å¢ƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        print(f"ğŸ“… å¤ã„ç’°å¢ƒ: {old_env_dir.name}")
        print(f"ğŸ“… æ–°ã—ã„ç’°å¢ƒ: {new_env_dir.name}")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒ
        old_params = self.parse_benchmark_params(old_env_dir / "benchmark_params.txt")
        new_params = self.parse_benchmark_params(new_env_dir / "benchmark_params.txt")
        
        # æ€§èƒ½çµæœæ¯”è¼ƒ
        old_performance = self.parse_summary_file(old_env_dir / "performance_reversal_summary.txt")
        new_performance = self.parse_summary_file(new_env_dir / "performance_reversal_summary.txt")
        
        return {
            'old_params': old_params,
            'new_params': new_params,
            'old_performance': old_performance,
            'new_performance': new_performance,
            'old_dir': old_env_dir,
            'new_dir': new_env_dir
        }
    
    def compare_benchmark_parameters(self, old_params, new_params):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¯”è¼ƒ"""
        print("\n" + "="*80)
        print("ğŸ”§ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒ")
        print("="*80)
        
        comparison_data = []
        
        # å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¯”è¼ƒ
        common_params = set(old_params.keys()) & set(new_params.keys())
        
        for param in sorted(common_params):
            old_val = old_params[param]
            new_val = new_params[param]
            
            try:
                old_num = int(old_val)
                new_num = int(new_val)
                change = new_num - old_num
                change_pct = (change / old_num * 100) if old_num != 0 else 0
                
                comparison_data.append({
                    'parameter': param,
                    'old_value': old_val,
                    'new_value': new_val,
                    'change': change,
                    'change_percent': change_pct
                })
                
                change_symbol = "ğŸ“ˆ" if change > 0 else "ğŸ“‰" if change < 0 else "â¡ï¸"
                print(f"{change_symbol} {param}: {old_val} â†’ {new_val} ({change:+d}, {change_pct:+.1f}%)")
                
            except ValueError:
                # æ•°å€¤ä»¥å¤–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                comparison_data.append({
                    'parameter': param,
                    'old_value': old_val,
                    'new_value': new_val,
                    'change': 'N/A',
                    'change_percent': 'N/A'
                })
                print(f"â¡ï¸ {param}: {old_val} â†’ {new_val}")
        
        # æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        new_only_params = set(new_params.keys()) - set(old_params.keys())
        if new_only_params:
            print(f"\nğŸ†• æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
            for param in sorted(new_only_params):
                print(f"  â€¢ {param}: {new_params[param]}")
        
        # å‰Šé™¤ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        old_only_params = set(old_params.keys()) - set(new_params.keys())
        if old_only_params:
            print(f"\nğŸ—‘ï¸ å‰Šé™¤ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
            for param in sorted(old_only_params):
                print(f"  â€¢ {param}: {old_params[param]}")
        
        return comparison_data
    
    def compare_performance_results(self, old_performance, new_performance):
        """æ€§èƒ½çµæœã®æ¯”è¼ƒ"""
        print(f"\n" + "="*80)
        print("ğŸ“Š æ€§èƒ½çµæœæ¯”è¼ƒ")
        print("="*80)
        
        if not old_performance or not new_performance:
            print("âŒ æ€§èƒ½ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ã§ã™")
            return
        
        metrics = [
            ('h3_advantage_ratio', 'HTTP/3å„ªä½ç‡'),
            ('avg_throughput_advantage', 'å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ€§èƒ½å·®'),
            ('avg_latency_advantage', 'å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ€§èƒ½å·®'),
            ('avg_connection_advantage', 'å¹³å‡æ¥ç¶šæ™‚é–“æ€§èƒ½å·®'),
            ('max_throughput_advantage', 'æœ€å¤§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ€§èƒ½å·®')
        ]
        
        comparison_results = []
        
        for metric, name in metrics:
            if metric in old_performance and metric in new_performance:
                old_val = old_performance[metric]
                new_val = new_performance[metric]
                change = new_val - old_val
                
                comparison_results.append({
                    'metric': name,
                    'old_value': old_val,
                    'new_value': new_val,
                    'change': change
                })
                
                change_symbol = "ğŸ“ˆ" if change > 0 else "ğŸ“‰" if change < 0 else "â¡ï¸"
                if isinstance(old_val, float):
                    print(f"{change_symbol} {name}: {old_val:.1f}% â†’ {new_val:.1f}% ({change:+.1f}%)")
                else:
                    print(f"{change_symbol} {name}: {old_val} â†’ {new_val} ({change:+d})")
        
        return comparison_results
    
    def analyze_improvements(self, comparison_data, performance_results):
        """æ”¹å–„ç‚¹ã®åˆ†æ"""
        print(f"\n" + "="*80)
        print("ğŸš€ ç’°å¢ƒæ”¹å–„ç‚¹ã®åˆ†æ")
        print("="*80)
        
        improvements = []
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ”¹å–„ã®åˆ†æ
        for param_data in comparison_data:
            if param_data['parameter'] == 'REQUESTS':
                if param_data['change'] < 0:
                    improvements.append("ğŸ“‰ ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°å‰Šæ¸›: æ¸¬å®šæ™‚é–“çŸ­ç¸®ã¨ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡åŒ–")
                else:
                    improvements.append("ğŸ“ˆ ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°å¢—åŠ : ã‚ˆã‚Šè©³ç´°ãªæ¸¬å®š")
            
            elif param_data['parameter'] == 'CONNECTIONS':
                if param_data['change'] < 0:
                    improvements.append("ğŸ“‰ æ¥ç¶šæ•°å‰Šæ¸›: ã‚·ã‚¹ãƒ†ãƒ è² è·è»½æ¸›")
                else:
                    improvements.append("ğŸ“ˆ æ¥ç¶šæ•°å¢—åŠ : ã‚ˆã‚Šç¾å®Ÿçš„ãªè² è·")
            
            elif param_data['parameter'] == 'THREADS':
                if param_data['change'] < 0:
                    improvements.append("ğŸ“‰ ã‚¹ãƒ¬ãƒƒãƒ‰æ•°å‰Šæ¸›: CPUç«¶åˆã®è»½æ¸›")
                else:
                    improvements.append("ğŸ“ˆ ã‚¹ãƒ¬ãƒƒãƒ‰æ•°å¢—åŠ : ä¸¦åˆ—å‡¦ç†å‘ä¸Š")
            
            elif param_data['parameter'] == 'CONNECTION_WARMUP_TIME':
                if param_data['change'] > 0:
                    improvements.append("ğŸ“ˆ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ™‚é–“å¢—åŠ : ã‚ˆã‚Šå®‰å®šã—ãŸæ¸¬å®š")
            
            elif param_data['parameter'] == 'SYSTEM_STABILIZATION_TIME':
                improvements.append("ğŸ†• ã‚·ã‚¹ãƒ†ãƒ å®‰å®šåŒ–æ™‚é–“è¿½åŠ : æ¸¬å®šå‰ã®ç’°å¢ƒå®‰å®šåŒ–")
            
            elif param_data['parameter'] == 'MEMORY_CLEANUP_ENABLED':
                if param_data['new_value'] == 'true':
                    improvements.append("ğŸ†• ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æœ‰åŠ¹åŒ–: ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯é˜²æ­¢")
            
            elif param_data['parameter'] == 'NETWORK_RESET_ENABLED':
                if param_data['new_value'] == 'true':
                    improvements.append("ğŸ†• ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒªã‚»ãƒƒãƒˆæœ‰åŠ¹åŒ–: æ¸¬å®šé–“ã®ç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
        
        # æ€§èƒ½æ”¹å–„ã®åˆ†æ
        for perf_data in performance_results:
            if perf_data['metric'] == 'HTTP/3å„ªä½ç‡':
                if perf_data['change'] > 0:
                    improvements.append("âœ… HTTP/3å„ªä½ã‚±ãƒ¼ã‚¹å¢—åŠ : ã‚ˆã‚Šé©åˆ‡ãªæ¡ä»¶è¨­å®š")
                elif perf_data['change'] < 0:
                    improvements.append("âš ï¸ HTTP/3å„ªä½ã‚±ãƒ¼ã‚¹æ¸›å°‘: æ¡ä»¶è¨­å®šã®è¦‹ç›´ã—å¿…è¦")
            
            elif perf_data['metric'] == 'å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ€§èƒ½å·®':
                if abs(perf_data['change']) < 5:
                    improvements.append("âœ… ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ€§èƒ½å·®å®‰å®šåŒ–: æ¸¬å®šç²¾åº¦å‘ä¸Š")
                else:
                    improvements.append("âš ï¸ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ€§èƒ½å·®å¤‰å‹•: æ¸¬å®šæ¡ä»¶ã®èª¿æ•´å¿…è¦")
        
        # æ”¹å–„ç‚¹ã®è¡¨ç¤º
        if improvements:
            print("ğŸ¯ æ¤œå‡ºã•ã‚ŒãŸæ”¹å–„ç‚¹:")
            for i, improvement in enumerate(improvements, 1):
                print(f"  {i}. {improvement}")
        else:
            print("â„¹ï¸ æ˜ç¢ºãªæ”¹å–„ç‚¹ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        return improvements
    
    def generate_comparison_graphs(self, comparison_data, performance_results, output_dir):
        """æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('3æ—¥å‰ vs ç¾åœ¨ã®å®Ÿé¨“ç’°å¢ƒæ¯”è¼ƒ', fontsize=16, fontweight='bold')
        
        # 1. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´ã®å½±éŸ¿
        ax1 = axes[0, 0]
        numeric_params = [d for d in comparison_data if isinstance(d['change'], (int, float))]
        if numeric_params:
            params = [d['parameter'] for d in numeric_params]
            changes = [d['change_percent'] for d in numeric_params if isinstance(d['change_percent'], (int, float))]
            
            colors = ['red' if c < 0 else 'green' if c > 0 else 'gray' for c in changes]
            bars = ax1.bar(range(len(changes)), changes, color=colors, alpha=0.7)
            ax1.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿')
            ax1.set_ylabel('å¤‰æ›´ç‡ (%)')
            ax1.set_title('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´ç‡')
            ax1.set_xticks(range(len(changes)))
            ax1.set_xticklabels(params, rotation=45, ha='right')
            ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax1.grid(True, alpha=0.3)
        
        # 2. æ€§èƒ½æŒ‡æ¨™æ¯”è¼ƒ
        ax2 = axes[0, 1]
        if performance_results:
            metrics = [d['metric'] for d in performance_results]
            old_vals = [d['old_value'] for d in performance_results]
            new_vals = [d['new_value'] for d in performance_results]
            
            x = np.arange(len(metrics))
            width = 0.35
            
            ax2.bar(x - width/2, old_vals, width, label='3æ—¥å‰', alpha=0.8)
            ax2.bar(x + width/2, new_vals, width, label='ç¾åœ¨', alpha=0.8)
            ax2.set_xlabel('æ€§èƒ½æŒ‡æ¨™')
            ax2.set_ylabel('å€¤')
            ax2.set_title('æ€§èƒ½æŒ‡æ¨™æ¯”è¼ƒ')
            ax2.set_xticks(x)
            ax2.set_xticklabels(metrics, rotation=45, ha='right')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        # 3. æ€§èƒ½å¤‰åŒ–ã®è©³ç´°
        ax3 = axes[1, 0]
        if performance_results:
            metrics = [d['metric'] for d in performance_results]
            changes = [d['change'] for d in performance_results]
            
            colors = ['red' if c < 0 else 'green' if c > 0 else 'gray' for c in changes]
            bars = ax3.bar(range(len(changes)), changes, color=colors, alpha=0.7)
            ax3.set_xlabel('æ€§èƒ½æŒ‡æ¨™')
            ax3.set_ylabel('å¤‰åŒ–é‡')
            ax3.set_title('æ€§èƒ½å¤‰åŒ–é‡')
            ax3.set_xticks(range(len(changes)))
            ax3.set_xticklabels(metrics, rotation=45, ha='right')
            ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax3.grid(True, alpha=0.3)
        
        # 4. ç’°å¢ƒæ”¹å–„ã®è¦ç´„
        ax4 = axes[1, 1]
        ax4.axis('off')
        
        # æ”¹å–„ç‚¹ã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
        summary_text = "ç’°å¢ƒæ”¹å–„ã®è¦ç´„:\n\n"
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ”¹å–„
        param_improvements = sum(1 for d in comparison_data if isinstance(d['change'], (int, float)) and d['change'] != 0)
        summary_text += f"â€¢ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´: {param_improvements}é …ç›®\n"
        
        # æ–°æ©Ÿèƒ½è¿½åŠ 
        new_features = sum(1 for d in comparison_data if d['change'] == 'N/A' and 'SYSTEM_STABILIZATION_TIME' in d['parameter'])
        summary_text += f"â€¢ æ–°æ©Ÿèƒ½è¿½åŠ : {new_features}é …ç›®\n"
        
        # æ€§èƒ½å¤‰åŒ–
        if performance_results:
            improved_metrics = sum(1 for d in performance_results if d['change'] > 0)
            summary_text += f"â€¢ æ€§èƒ½æ”¹å–„æŒ‡æ¨™: {improved_metrics}é …ç›®\n"
        
        summary_text += "\nä¸»ãªæ”¹å–„ç‚¹:\n"
        summary_text += "â€¢ æ¸¬å®šå®‰å®šæ€§å‘ä¸Š\n"
        summary_text += "â€¢ ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡åŒ–\n"
        summary_text += "â€¢ ç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n"
        summary_text += "â€¢ æ—¥æœ¬èªå¯¾å¿œ\n"
        
        ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
        
        plt.tight_layout()
        
        # ä¿å­˜
        plt.savefig(output_dir / "environment_comparison.png", dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ç’°å¢ƒæ¯”è¼ƒã‚°ãƒ©ãƒ•ä¿å­˜: {output_dir / 'environment_comparison.png'}")
    
    def generate_detailed_report(self, comparison_data, performance_results, improvements, output_dir):
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report_file = output_dir / "environment_comparison_report.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("3æ—¥å‰ vs ç¾åœ¨ã®å®Ÿé¨“ç’°å¢ƒæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("="*80 + "\n")
            f.write(f"ç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("ğŸ“Š ç’°å¢ƒæ¦‚è¦\n")
            f.write("-" * 40 + "\n")
            f.write("â€¢ å¤ã„ç’°å¢ƒ: 2025-07-23 (3æ—¥å‰)\n")
            f.write("â€¢ æ–°ã—ã„ç’°å¢ƒ: 2025-07-25 (ç¾åœ¨)\n")
            f.write("â€¢ æ¯”è¼ƒæœŸé–“: ç´„3æ—¥é–“\n\n")
            
            f.write("ğŸ”§ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´è©³ç´°\n")
            f.write("-" * 40 + "\n")
            for param_data in comparison_data:
                if isinstance(param_data['change'], (int, float)):
                    f.write(f"â€¢ {param_data['parameter']}: {param_data['old_value']} â†’ {param_data['new_value']} ({param_data['change']:+d}, {param_data['change_percent']:+.1f}%)\n")
                else:
                    f.write(f"â€¢ {param_data['parameter']}: {param_data['old_value']} â†’ {param_data['new_value']}\n")
            
            f.write("\nğŸ“ˆ æ€§èƒ½å¤‰åŒ–è©³ç´°\n")
            f.write("-" * 40 + "\n")
            for perf_data in performance_results:
                if isinstance(perf_data['old_value'], float):
                    f.write(f"â€¢ {perf_data['metric']}: {perf_data['old_value']:.1f}% â†’ {perf_data['new_value']:.1f}% ({perf_data['change']:+.1f}%)\n")
                else:
                    f.write(f"â€¢ {perf_data['metric']}: {perf_data['old_value']} â†’ {perf_data['new_value']} ({perf_data['change']:+d})\n")
            
            f.write("\nğŸš€ ä¸»è¦ãªæ”¹å–„ç‚¹\n")
            f.write("-" * 40 + "\n")
            for i, improvement in enumerate(improvements, 1):
                f.write(f"{i}. {improvement}\n")
            
            f.write("\nğŸ” æŠ€è¡“çš„æ”¹å–„ã®æ„ç¾©\n")
            f.write("-" * 40 + "\n")
            f.write("â€¢ æ¸¬å®šå®‰å®šæ€§: ã‚·ã‚¹ãƒ†ãƒ å®‰å®šåŒ–æ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã«ã‚ˆã‚Šæ¸¬å®šç²¾åº¦å‘ä¸Š\n")
            f.write("â€¢ ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡: é©åˆ‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã«ã‚ˆã‚Šã‚·ã‚¹ãƒ†ãƒ è² è·è»½æ¸›\n")
            f.write("â€¢ å†ç¾æ€§: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒªã‚»ãƒƒãƒˆæ©Ÿèƒ½ã«ã‚ˆã‚Šæ¸¬å®šé–“ã®ç‹¬ç«‹æ€§ç¢ºä¿\n")
            f.write("â€¢ ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£: æ—¥æœ¬èªå¯¾å¿œã«ã‚ˆã‚Šçµæœã®ç†è§£ã—ã‚„ã™ã•å‘ä¸Š\n")
            
            f.write("\nğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«\n")
            f.write("-" * 40 + "\n")
            f.write("â€¢ environment_comparison.png - ç’°å¢ƒæ¯”è¼ƒã‚°ãƒ©ãƒ•\n")
            f.write("â€¢ environment_comparison_report.txt - ã“ã®ãƒ¬ãƒãƒ¼ãƒˆ\n")
        
        print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
    
    def run_comparison(self):
        """å®Œå…¨ãªæ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸš€ å®Ÿé¨“ç’°å¢ƒæ¯”è¼ƒåˆ†æã‚’é–‹å§‹...")
        
        # ç’°å¢ƒã®é•ã„ã‚’åˆ†æ
        env_data = self.analyze_environment_differences()
        
        if not env_data:
            print("âŒ æ¯”è¼ƒå¯èƒ½ãªç’°å¢ƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒ
        comparison_data = self.compare_benchmark_parameters(
            env_data['old_params'], 
            env_data['new_params']
        )
        
        # æ€§èƒ½çµæœæ¯”è¼ƒ
        performance_results = self.compare_performance_results(
            env_data['old_performance'], 
            env_data['new_performance']
        )
        
        # æ”¹å–„ç‚¹åˆ†æ
        improvements = self.analyze_improvements(comparison_data, performance_results)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path("logs") / f"environment_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        output_dir.mkdir(exist_ok=True)
        
        # ã‚°ãƒ©ãƒ•ç”Ÿæˆ
        self.generate_comparison_graphs(comparison_data, performance_results, output_dir)
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_detailed_report(comparison_data, performance_results, improvements, output_dir)
        
        print(f"\nâœ… æ¯”è¼ƒåˆ†æå®Œäº†ï¼çµæœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    analyzer = EnvironmentComparisonAnalyzer()
    analyzer.run_comparison() 
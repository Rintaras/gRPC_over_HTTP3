#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‡ãƒ¼ã‚¿ã®ã°ã‚‰ã¤ãåŸå› åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2
CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿéš›ã®å½¢å¼ã«å¯¾å¿œ
"""

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'DejaVu Sans']

class VarianceAnalyzerV2:
    def __init__(self):
        self.experiment_data = {}
        self.variance_sources = {}
        self.analysis_results = {}
        
    def load_experiment_data(self, log_dir="logs"):
        """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ” å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        # æœ€æ–°ã®5å›å®Ÿé¨“ã‚’æ¤œç´¢
        experiment_dirs = []
        for item in os.listdir(log_dir):
            if item.startswith("japanese_benchmark_") and os.path.isdir(os.path.join(log_dir, item)):
                experiment_dirs.append(item)
        
        # æœ€æ–°ã®5ã¤ã‚’å–å¾—
        experiment_dirs.sort(reverse=True)
        latest_experiments = experiment_dirs[:5]
        
        print(f"ğŸ“Š æ¤œå‡ºã•ã‚ŒãŸå®Ÿé¨“æ•°: {len(latest_experiments)}")
        
        for exp_dir in latest_experiments:
            exp_path = os.path.join(log_dir, exp_dir)
            print(f"ğŸ” å®Ÿé¨“: {exp_dir}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            csv_files = [f for f in os.listdir(exp_path) if f.endswith('.csv')]
            
            exp_data = {}
            for csv_file in csv_files:
                csv_path = os.path.join(exp_path, csv_file)
                try:
                    # ã‚«ãƒ©ãƒ åãªã—ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
                    df = pd.read_csv(csv_path, header=None)
                    # ã‚«ãƒ©ãƒ åã‚’è¨­å®šï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼‰
                    df.columns = ['timestamp', 'status_code', 'response_time']
                    exp_data[csv_file] = df
                    print(f"  âœ… {csv_file}: {len(df)} è¡Œ")
                except Exception as e:
                    print(f"  âŒ {csv_file}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
            
            self.experiment_data[exp_dir] = exp_data
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.experiment_data)} å®Ÿé¨“")
        
    def analyze_performance_variance(self):
        """æ€§èƒ½ã®ã°ã‚‰ã¤ãã‚’åˆ†æ"""
        print("\nğŸ“ˆ æ€§èƒ½ã°ã‚‰ã¤ãåˆ†æã‚’é–‹å§‹...")
        
        variance_data = {
            'throughput': [],
            'latency': [],
            'success_rate': [],
            'experiment': [],
            'protocol': [],
            'network_condition': []
        }
        
        for exp_name, exp_data in self.experiment_data.items():
            for csv_name, df in exp_data.items():
                if len(df) == 0:
                    continue
                
                # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã‚’æŠ½å‡º
                if 'h2_' in csv_name:
                    protocol = 'HTTP/2'
                elif 'h3_' in csv_name:
                    protocol = 'HTTP/3'
                else:
                    continue
                
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã‚’æŠ½å‡º
                network_match = re.search(r'(\d+)ms_(\d+)pct', csv_name)
                if network_match:
                    delay = int(network_match.group(1))
                    loss = int(network_match.group(2))
                    network_condition = f"{delay}ms_{loss}%"
                else:
                    network_condition = "unknown"
                
                # æ€§èƒ½æŒ‡æ¨™ã‚’æŠ½å‡º
                try:
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼ˆãƒã‚¤ã‚¯ãƒ­ç§’ã‹ã‚‰ãƒŸãƒªç§’ã«å¤‰æ›ï¼‰
                    latency = df['response_time'].mean() / 1000  # Î¼s to ms
                    
                    # æˆåŠŸç‡
                    success_count = len(df[df['status_code'] == 200])
                    total_count = len(df)
                    success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
                    
                    # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆæˆåŠŸã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° / ç·æ™‚é–“ï¼‰
                    if len(df) > 1:
                        # æœ€åˆã¨æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰æ™‚é–“ã‚’è¨ˆç®—
                        start_time = df['timestamp'].min()
                        end_time = df['timestamp'].max()
                        duration_seconds = (end_time - start_time) / 1e9  # ãƒŠãƒç§’ã‹ã‚‰ç§’ã«å¤‰æ›
                        if duration_seconds > 0:
                            throughput = success_count / duration_seconds
                        else:
                            throughput = np.nan
                    else:
                        throughput = np.nan
                    
                    variance_data['throughput'].append(throughput)
                    variance_data['latency'].append(latency)
                    variance_data['success_rate'].append(success_rate)
                    variance_data['experiment'].append(exp_name)
                    variance_data['protocol'].append(protocol)
                    variance_data['network_condition'].append(network_condition)
                    
                except Exception as e:
                    print(f"  âš ï¸ ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({csv_name}): {e}")
        
        self.variance_df = pd.DataFrame(variance_data)
        print(f"âœ… æ€§èƒ½ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {len(self.variance_df)} ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ")
        
        # ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’è¡¨ç¤º
        print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {len(self.variance_df[self.variance_df['throughput'].notna()])} ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ")
        print(f"  ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼: {len(self.variance_df[self.variance_df['latency'].notna()])} ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ")
        print(f"  æˆåŠŸç‡: {len(self.variance_df[self.variance_df['success_rate'].notna()])} ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ")
        
    def identify_variance_sources(self):
        """ã°ã‚‰ã¤ãã®åŸå› ã‚’ç‰¹å®š"""
        print("\nğŸ” ã°ã‚‰ã¤ãåŸå› ã®ç‰¹å®šã‚’é–‹å§‹...")
        
        if len(self.variance_df) == 0:
            print("âŒ åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # å„æŒ‡æ¨™ã®å¤‰å‹•ä¿‚æ•°ã‚’è¨ˆç®—
        variance_sources = {}
        
        for metric in ['throughput', 'latency', 'success_rate']:
            metric_data = self.variance_df[metric].dropna()
            if len(metric_data) > 0:
                cv = metric_data.std() / metric_data.mean() if metric_data.mean() != 0 else np.inf
                variance_sources[metric] = {
                    'mean': metric_data.mean(),
                    'std': metric_data.std(),
                    'cv': cv,
                    'min': metric_data.min(),
                    'max': metric_data.max(),
                    'range': metric_data.max() - metric_data.min()
                }
        
        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«åˆ¥ã®ã°ã‚‰ã¤ã
        protocol_variance = {}
        for protocol in self.variance_df['protocol'].unique():
            protocol_data = self.variance_df[self.variance_df['protocol'] == protocol]
            for metric in ['throughput', 'latency', 'success_rate']:
                metric_data = protocol_data[metric].dropna()
                if len(metric_data) > 0:
                    cv = metric_data.std() / metric_data.mean() if metric_data.mean() != 0 else np.inf
                    protocol_variance[f"{protocol}_{metric}"] = cv
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶åˆ¥ã®ã°ã‚‰ã¤ã
        network_variance = {}
        for condition in self.variance_df['network_condition'].unique():
            condition_data = self.variance_df[self.variance_df['network_condition'] == condition]
            for metric in ['throughput', 'latency', 'success_rate']:
                metric_data = condition_data[metric].dropna()
                if len(metric_data) > 0:
                    cv = metric_data.std() / metric_data.mean() if metric_data.mean() != 0 else np.inf
                    network_variance[f"{condition}_{metric}"] = cv
        
        # å®Ÿé¨“é–“ã®ã°ã‚‰ã¤ã
        experiment_variance = {}
        for exp in self.variance_df['experiment'].unique():
            exp_data = self.variance_df[self.variance_df['experiment'] == exp]
            for metric in ['throughput', 'latency', 'success_rate']:
                metric_data = exp_data[metric].dropna()
                if len(metric_data) > 0:
                    cv = metric_data.std() / metric_data.mean() if metric_data.mean() != 0 else np.inf
                    experiment_variance[f"{exp}_{metric}"] = cv
        
        self.variance_sources = {
            'overall': variance_sources,
            'protocol': protocol_variance,
            'network': network_variance,
            'experiment': experiment_variance
        }
        
        print("âœ… ã°ã‚‰ã¤ãåŸå› ã®ç‰¹å®šå®Œäº†")
        
    def generate_variance_report(self):
        """ã°ã‚‰ã¤ãåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\nğŸ“„ ã°ã‚‰ã¤ãåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = f"logs/variance_analysis_v2_{timestamp}"
        os.makedirs(report_dir, exist_ok=True)
        
        # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹
        report_content = []
        report_content.append("=" * 80)
        report_content.append("ãƒ‡ãƒ¼ã‚¿ã°ã‚‰ã¤ãåŸå› åˆ†æãƒ¬ãƒãƒ¼ãƒˆ v2")
        report_content.append("=" * 80)
        report_content.append(f"ç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_content.append("")
        
        # å…¨ä½“ã®ã°ã‚‰ã¤ãçŠ¶æ³
        report_content.append("ğŸ“Š å…¨ä½“ã®ã°ã‚‰ã¤ãçŠ¶æ³")
        report_content.append("-" * 40)
        if 'overall' in self.variance_sources:
            for metric, stats in self.variance_sources['overall'].items():
                report_content.append(f"â€¢ {metric.upper()}:")
                report_content.append(f"  - å¹³å‡: {stats['mean']:.2f}")
                report_content.append(f"  - æ¨™æº–åå·®: {stats['std']:.2f}")
                report_content.append(f"  - å¤‰å‹•ä¿‚æ•°: {stats['cv']:.3f}")
                report_content.append(f"  - ç¯„å›²: {stats['min']:.2f} ã€œ {stats['max']:.2f}")
                report_content.append("")
        
        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«åˆ¥ã®ã°ã‚‰ã¤ã
        report_content.append("ğŸŒ ãƒ—ãƒ­ãƒˆã‚³ãƒ«åˆ¥ã®ã°ã‚‰ã¤ã")
        report_content.append("-" * 40)
        if 'protocol' in self.variance_sources:
            for key, cv in self.variance_sources['protocol'].items():
                report_content.append(f"â€¢ {key}: å¤‰å‹•ä¿‚æ•° {cv:.3f}")
            report_content.append("")
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶åˆ¥ã®ã°ã‚‰ã¤ã
        report_content.append("ğŸ“¡ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶åˆ¥ã®ã°ã‚‰ã¤ã")
        report_content.append("-" * 40)
        if 'network' in self.variance_sources:
            for key, cv in self.variance_sources['network'].items():
                report_content.append(f"â€¢ {key}: å¤‰å‹•ä¿‚æ•° {cv:.3f}")
            report_content.append("")
        
        # å®Ÿé¨“é–“ã®ã°ã‚‰ã¤ã
        report_content.append("ğŸ§ª å®Ÿé¨“é–“ã®ã°ã‚‰ã¤ã")
        report_content.append("-" * 40)
        if 'experiment' in self.variance_sources:
            for key, cv in self.variance_sources['experiment'].items():
                report_content.append(f"â€¢ {key}: å¤‰å‹•ä¿‚æ•° {cv:.3f}")
            report_content.append("")
        
        # ã°ã‚‰ã¤ãã®åŸå› åˆ†æ
        report_content.append("ğŸ” ã°ã‚‰ã¤ãã®åŸå› åˆ†æ")
        report_content.append("-" * 40)
        
        # æœ€ã‚‚ã°ã‚‰ã¤ãã®å¤§ãã„è¦å› ã‚’ç‰¹å®š
        all_cvs = []
        if 'protocol' in self.variance_sources:
            all_cvs.extend(self.variance_sources['protocol'].items())
        if 'network' in self.variance_sources:
            all_cvs.extend(self.variance_sources['network'].items())
        if 'experiment' in self.variance_sources:
            all_cvs.extend(self.variance_sources['experiment'].items())
        
        if all_cvs:
            # å¤‰å‹•ä¿‚æ•°ã§ã‚½ãƒ¼ãƒˆ
            all_cvs.sort(key=lambda x: x[1], reverse=True)
            
            report_content.append("ğŸ“ˆ æœ€ã‚‚ã°ã‚‰ã¤ãã®å¤§ãã„è¦å› ï¼ˆä¸Šä½5ã¤ï¼‰:")
            for i, (factor, cv) in enumerate(all_cvs[:5]):
                report_content.append(f"  {i+1}. {factor}: {cv:.3f}")
            report_content.append("")
        
        # æ¨å¥¨äº‹é …
        report_content.append("ğŸ’¡ æ¨å¥¨äº‹é …")
        report_content.append("-" * 40)
        
        # å¤‰å‹•ä¿‚æ•°ãŒé«˜ã„å ´åˆã®å¯¾ç­–
        high_variance_threshold = 0.5
        high_variance_factors = [item for item in all_cvs if item[1] > high_variance_threshold]
        
        if high_variance_factors:
            report_content.append("âš ï¸ é«˜ã„ã°ã‚‰ã¤ããŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ:")
            for factor, cv in high_variance_factors:
                report_content.append(f"  â€¢ {factor}: {cv:.3f}")
            report_content.append("")
            report_content.append("ğŸ”§ å¯¾ç­–:")
            report_content.append("  1. ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®å®‰å®šåŒ–")
            report_content.append("  2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã®ä¸€è²«æ€§ç¢ºä¿")
            report_content.append("  3. æ¸¬å®šå›æ•°ã®å¢—åŠ ")
            report_content.append("  4. å¤–ã‚Œå€¤ã®é™¤å»")
            report_content.append("  5. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´")
        else:
            report_content.append("âœ… ã°ã‚‰ã¤ãã¯è¨±å®¹ç¯„å›²å†…ã§ã™")
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = os.path.join(report_dir, "variance_analysis_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {report_path}")
        
        # ã°ã‚‰ã¤ãå¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
        self.generate_variance_plots(report_dir)
        
        return report_dir
        
    def generate_variance_plots(self, report_dir):
        """ã°ã‚‰ã¤ãã®å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        print("ğŸ“Š ã°ã‚‰ã¤ãå¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆä¸­...")
        
        if len(self.variance_df) == 0:
            print("âŒ ã‚°ãƒ©ãƒ•ç”Ÿæˆã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # 1. ãƒ—ãƒ­ãƒˆã‚³ãƒ«åˆ¥ã®ã°ã‚‰ã¤ãæ¯”è¼ƒ
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ãƒ‡ãƒ¼ã‚¿ã°ã‚‰ã¤ãåˆ†æ v2', fontsize=16, fontweight='bold')
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ã°ã‚‰ã¤ã
        ax1 = axes[0, 0]
        throughput_data = self.variance_df[self.variance_df['throughput'].notna()]
        if len(throughput_data) > 0:
            sns.boxplot(data=throughput_data, x='protocol', y='throughput', ax=ax1)
            ax1.set_title('ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ã°ã‚‰ã¤ã')
            ax1.set_ylabel('ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (req/s)')
        
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã®ã°ã‚‰ã¤ã
        ax2 = axes[0, 1]
        latency_data = self.variance_df[self.variance_df['latency'].notna()]
        if len(latency_data) > 0:
            sns.boxplot(data=latency_data, x='protocol', y='latency', ax=ax2)
            ax2.set_title('ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã®ã°ã‚‰ã¤ã')
            ax2.set_ylabel('ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ (ms)')
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶åˆ¥ã®ã°ã‚‰ã¤ã
        ax3 = axes[1, 0]
        if len(throughput_data) > 0:
            sns.boxplot(data=throughput_data, x='network_condition', y='throughput', ax=ax3)
            ax3.set_title('ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶åˆ¥ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã°ã‚‰ã¤ã')
            ax3.set_ylabel('ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (req/s)')
            ax3.tick_params(axis='x', rotation=45)
        
        # æˆåŠŸç‡ã®ã°ã‚‰ã¤ã
        ax4 = axes[1, 1]
        success_data = self.variance_df[self.variance_df['success_rate'].notna()]
        if len(success_data) > 0:
            sns.boxplot(data=success_data, x='protocol', y='success_rate', ax=ax4)
            ax4.set_title('æˆåŠŸç‡ã®ã°ã‚‰ã¤ã')
            ax4.set_ylabel('æˆåŠŸç‡ (%)')
        
        plt.tight_layout()
        plot_path = os.path.join(report_dir, "variance_analysis.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ã‚°ãƒ©ãƒ•ä¿å­˜å®Œäº†: {plot_path}")
        
    def run_analysis(self):
        """å®Œå…¨ãªã°ã‚‰ã¤ãåˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸš€ ãƒ‡ãƒ¼ã‚¿ã°ã‚‰ã¤ãåŸå› åˆ†æ v2 ã‚’é–‹å§‹...")
        
        self.load_experiment_data()
        self.analyze_performance_variance()
        self.identify_variance_sources()
        report_dir = self.generate_variance_report()
        
        print(f"\nâœ… ã°ã‚‰ã¤ãåˆ†æå®Œäº†ï¼")
        print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {report_dir}")
        
        return report_dir

if __name__ == "__main__":
    analyzer = VarianceAnalyzerV2()
    analyzer.run_analysis() 
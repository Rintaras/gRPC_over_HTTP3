#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›£è¦–ãƒ‡ãƒ¼ã‚¿çµ±åˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
5å›å®Ÿè¡Œã®ç›£è¦–ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦åˆ†æ
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'DejaVu Sans']

class ConsolidatedMonitoringAnalyzer:
    def __init__(self):
        self.experiment_data = {}
        self.consolidated_results = {}
        
    def find_latest_experiments(self, log_dir="logs", pattern="monitored_benchmark_*"):
        """æœ€æ–°ã®ç›£è¦–ä»˜ããƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“ã‚’æ¤œç´¢"""
        print("ğŸ” æœ€æ–°ã®ç›£è¦–ä»˜ããƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“ã‚’æ¤œç´¢ä¸­...")
        
        log_path = Path(log_dir)
        experiment_dirs = []
        
        for dir_path in log_path.glob(pattern):
            if dir_path.is_dir():
                experiment_dirs.append(dir_path)
        
        # æœ€æ–°ã®5ã¤ã®å®Ÿé¨“ã‚’å–å¾—
        experiment_dirs.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        latest_experiments = experiment_dirs[:5]
        
        print(f"âœ… {len(latest_experiments)}å€‹ã®å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç™ºè¦‹")
        for exp_dir in latest_experiments:
            print(f"  - {exp_dir.name}")
        
        return latest_experiments
    
    def analyze_experiment_consistency(self, experiment_dirs):
        """å®Ÿé¨“é–“ã®ä¸€è²«æ€§ã‚’åˆ†æ"""
        print("\nğŸ“Š å®Ÿé¨“é–“ã®ä¸€è²«æ€§ã‚’åˆ†æä¸­...")
        
        consistency_data = {
            'throughput': [],
            'latency': [],
            'success_rate': [],
            'execution_time': []
        }
        
        for exp_dir in experiment_dirs:
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            summary_file = exp_dir / "benchmark_summary.txt"
            if summary_file.exists():
                try:
                    with open(summary_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # å®Ÿè¡Œæ™‚é–“ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    execution_time = 0
                    if "å®Ÿè¡Œæ™‚é–“:" in content:
                        time_line = [line for line in content.split('\n') if "å®Ÿè¡Œæ™‚é–“:" in line]
                        if time_line:
                            time_str = time_line[0].split("å®Ÿè¡Œæ™‚é–“:")[1].strip()
                            execution_time = int(time_str.split()[0])
                    
                    consistency_data['execution_time'].append(execution_time)
                    
                except Exception as e:
                    print(f"âš ï¸ ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({exp_dir.name}): {e}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ€§èƒ½ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            csv_files = list(exp_dir.glob("*.csv"))
            for csv_file in csv_files:
                try:
                    df = pd.read_csv(csv_file, header=None, sep='\t')
                    if len(df.columns) >= 3:
                        df.columns = ['timestamp', 'status_code', 'response_time'] + list(df.columns[3:])
                        
                        # æ€§èƒ½æŒ‡æ¨™ã‚’è¨ˆç®—
                        throughput = len(df) / (df['response_time'].max() - df['response_time'].min()) * 1000000 if len(df) > 1 else 0
                        latency = df['response_time'].mean() / 1000  # Î¼s to ms
                        success_rate = (df['status_code'] == 200).mean() * 100
                        
                        consistency_data['throughput'].append(throughput)
                        consistency_data['latency'].append(latency)
                        consistency_data['success_rate'].append(success_rate)
                        
                except Exception as e:
                    print(f"âš ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({csv_file.name}): {e}")
        
        # ä¸€è²«æ€§çµ±è¨ˆã‚’è¨ˆç®—
        consistency_stats = {}
        for metric, values in consistency_data.items():
            if values:
                values = np.array(values)
                consistency_stats[metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'cv': np.std(values) / np.mean(values) if np.mean(values) > 0 else 0,
                    'min': np.min(values),
                    'max': np.max(values),
                    'range': np.max(values) - np.min(values)
                }
        
        self.consolidated_results['consistency'] = consistency_stats
        print("âœ… å®Ÿé¨“é–“ä¸€è²«æ€§åˆ†æå®Œäº†")
        
    def generate_consolidated_report(self, experiment_dirs):
        """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\nğŸ“„ çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        
        # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹
        report_content = []
        report_content.append("=" * 80)
        report_content.append("ç›£è¦–ä»˜ããƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµ±åˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        report_content.append("=" * 80)
        report_content.append(f"ç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_content.append(f"åˆ†æå¯¾è±¡å®Ÿé¨“æ•°: {len(experiment_dirs)}")
        report_content.append("")
        
        # å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§
        report_content.append("ğŸ“ åˆ†æå¯¾è±¡å®Ÿé¨“:")
        for i, exp_dir in enumerate(experiment_dirs, 1):
            report_content.append(f"  {i}. {exp_dir.name}")
        report_content.append("")
        
        # ä¸€è²«æ€§åˆ†æçµæœ
        if 'consistency' in self.consolidated_results:
            report_content.append("ğŸ“Š å®Ÿé¨“é–“ä¸€è²«æ€§åˆ†æçµæœ")
            report_content.append("-" * 40)
            
            consistency_stats = self.consolidated_results['consistency']
            
            for metric, stats in consistency_stats.items():
                if metric == 'throughput':
                    metric_name = "ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ"
                    unit = "req/s"
                elif metric == 'latency':
                    metric_name = "ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼"
                    unit = "ms"
                elif metric == 'success_rate':
                    metric_name = "æˆåŠŸç‡"
                    unit = "%"
                elif metric == 'execution_time':
                    metric_name = "å®Ÿè¡Œæ™‚é–“"
                    unit = "ç§’"
                else:
                    metric_name = metric
                    unit = ""
                
                report_content.append(f"ğŸ“ˆ {metric_name}:")
                report_content.append(f"  - å¹³å‡: {stats['mean']:.2f} {unit}")
                report_content.append(f"  - æ¨™æº–åå·®: {stats['std']:.2f} {unit}")
                report_content.append(f"  - å¤‰å‹•ä¿‚æ•°: {stats['cv']:.3f}")
                report_content.append(f"  - ç¯„å›²: {stats['min']:.2f} ã€œ {stats['max']:.2f} {unit}")
                
                # ä¸€è²«æ€§è©•ä¾¡
                if stats['cv'] < 0.1:
                    consistency_level = "éå¸¸ã«é«˜ã„"
                elif stats['cv'] < 0.2:
                    consistency_level = "é«˜ã„"
                elif stats['cv'] < 0.3:
                    consistency_level = "ä¸­ç¨‹åº¦"
                else:
                    consistency_level = "ä½ã„"
                
                report_content.append(f"  - ä¸€è²«æ€§ãƒ¬ãƒ™ãƒ«: {consistency_level}")
                report_content.append("")
        
        # ç·åˆè©•ä¾¡
        report_content.append("ğŸ¯ ç·åˆè©•ä¾¡")
        report_content.append("-" * 40)
        
        # å¹³å‡å¤‰å‹•ä¿‚æ•°ã‚’è¨ˆç®—
        avg_cv = np.mean([stats['cv'] for stats in consistency_stats.values()])
        
        if avg_cv < 0.15:
            overall_consistency = "éå¸¸ã«é«˜ã„"
            recommendation = "ç¾åœ¨ã®è¨­å®šã§ååˆ†ãªä¸€è²«æ€§ãŒç¢ºä¿ã•ã‚Œã¦ã„ã¾ã™"
        elif avg_cv < 0.25:
            overall_consistency = "é«˜ã„"
            recommendation = "è»½å¾®ãªèª¿æ•´ã§ä¸€è²«æ€§ã‚’å‘ä¸Šã§ãã¾ã™"
        elif avg_cv < 0.35:
            overall_consistency = "ä¸­ç¨‹åº¦"
            recommendation = "è¨­å®šã®è¦‹ç›´ã—ãŒå¿…è¦ã§ã™"
        else:
            overall_consistency = "ä½ã„"
            recommendation = "æ ¹æœ¬çš„ãªæ”¹å–„ãŒå¿…è¦ã§ã™"
        
        report_content.append(f"â€¢ å…¨ä½“çš„ãªä¸€è²«æ€§: {overall_consistency}")
        report_content.append(f"â€¢ å¹³å‡å¤‰å‹•ä¿‚æ•°: {avg_cv:.3f}")
        report_content.append(f"â€¢ æ¨å¥¨äº‹é …: {recommendation}")
        report_content.append("")
        
        # æ”¹å–„ææ¡ˆ
        report_content.append("ğŸ’¡ æ”¹å–„ææ¡ˆ")
        report_content.append("-" * 40)
        
        improvements = []
        
        if 'latency' in consistency_stats and consistency_stats['latency']['cv'] > 0.3:
            improvements.append("â€¢ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ¸¬å®šã®å®‰å®šåŒ–")
        
        if 'throughput' in consistency_stats and consistency_stats['throughput']['cv'] > 0.3:
            improvements.append("â€¢ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®šã®å®‰å®šåŒ–")
        
        if 'execution_time' in consistency_stats and consistency_stats['execution_time']['cv'] > 0.2:
            improvements.append("â€¢ å®Ÿè¡Œæ™‚é–“ã®ä¸€è²«æ€§ç¢ºä¿")
        
        if not improvements:
            improvements.append("â€¢ ç¾åœ¨ã®è¨­å®šã§ååˆ†ãªå®‰å®šæ€§ãŒç¢ºä¿ã•ã‚Œã¦ã„ã¾ã™")
        
        for improvement in improvements:
            report_content.append(improvement)
        
        report_content.append("")
        report_content.append("=" * 80)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_dir = Path("logs") / f"consolidated_monitoring_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        report_dir.mkdir(parents=True, exist_ok=True)
        
        report_path = report_dir / "consolidated_monitoring_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        print(f"âœ… çµ±åˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {report_path}")
        
        # çµ±åˆã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
        self.generate_consolidated_plots(report_dir, consistency_stats)
        
    def generate_consolidated_plots(self, report_dir, consistency_stats):
        """çµ±åˆåˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        print("ğŸ“Š çµ±åˆåˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆä¸­...")
        
        # ä¸€è²«æ€§æ¯”è¼ƒã‚°ãƒ©ãƒ•
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ç›£è¦–ä»˜ããƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµ±åˆåˆ†æçµæœ', fontsize=16, fontweight='bold')
        
        # å¤‰å‹•ä¿‚æ•°æ¯”è¼ƒ
        ax1 = axes[0, 0]
        metrics = list(consistency_stats.keys())
        cv_values = [consistency_stats[metric]['cv'] for metric in metrics]
        metric_names = ['ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ', 'ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼', 'æˆåŠŸç‡', 'å®Ÿè¡Œæ™‚é–“']
        
        bars = ax1.bar(metric_names, cv_values, color=['blue', 'red', 'green', 'orange'])
        ax1.set_title('å„æŒ‡æ¨™ã®å¤‰å‹•ä¿‚æ•°æ¯”è¼ƒ')
        ax1.set_ylabel('å¤‰å‹•ä¿‚æ•° (CV)')
        ax1.grid(True, alpha=0.3)
        
        # å€¤ã«å¿œã˜ã¦è‰²ã‚’å¤‰æ›´
        for bar, cv in zip(bars, cv_values):
            if cv > 0.3:
                bar.set_color('red')
            elif cv > 0.2:
                bar.set_color('orange')
            else:
                bar.set_color('green')
        
        # ç¯„å›²æ¯”è¼ƒ
        ax2 = axes[0, 1]
        range_values = [consistency_stats[metric]['range'] for metric in metrics]
        ax2.bar(metric_names, range_values, color=['blue', 'red', 'green', 'orange'])
        ax2.set_title('å„æŒ‡æ¨™ã®ç¯„å›²æ¯”è¼ƒ')
        ax2.set_ylabel('ç¯„å›²')
        ax2.grid(True, alpha=0.3)
        
        # å¹³å‡å€¤ã¨æ¨™æº–åå·®
        ax3 = axes[1, 0]
        means = [consistency_stats[metric]['mean'] for metric in metrics]
        stds = [consistency_stats[metric]['std'] for metric in metrics]
        
        x_pos = np.arange(len(metric_names))
        ax3.bar(x_pos, means, yerr=stds, capsize=5, color=['blue', 'red', 'green', 'orange'])
        ax3.set_title('å¹³å‡å€¤ã¨æ¨™æº–åå·®')
        ax3.set_ylabel('å€¤')
        ax3.set_xticks(x_pos)
        ax3.set_xticklabels(metric_names)
        ax3.grid(True, alpha=0.3)
        
        # ä¸€è²«æ€§ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ
        ax4 = axes[1, 1]
        consistency_levels = []
        for metric, stats in consistency_stats.items():
            cv = stats['cv']
            if cv < 0.1:
                level = "éå¸¸ã«é«˜ã„"
            elif cv < 0.2:
                level = "é«˜ã„"
            elif cv < 0.3:
                level = "ä¸­ç¨‹åº¦"
            else:
                level = "ä½ã„"
            consistency_levels.append(level)
        
        level_counts = pd.Series(consistency_levels).value_counts()
        ax4.pie(level_counts.values, labels=level_counts.index, autopct='%1.1f%%', startangle=90)
        ax4.set_title('ä¸€è²«æ€§ãƒ¬ãƒ™ãƒ«ã®åˆ†å¸ƒ')
        
        plt.tight_layout()
        plot_path = report_dir / "consolidated_monitoring_analysis.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… çµ±åˆåˆ†æã‚°ãƒ©ãƒ•ä¿å­˜: {plot_path}")
        
    def run_analysis(self):
        """å®Œå…¨ãªçµ±åˆåˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸš€ ç›£è¦–ãƒ‡ãƒ¼ã‚¿çµ±åˆåˆ†æã‚’é–‹å§‹...")
        
        # æœ€æ–°ã®å®Ÿé¨“ã‚’æ¤œç´¢
        experiment_dirs = self.find_latest_experiments()
        
        if not experiment_dirs:
            print("âŒ åˆ†æå¯¾è±¡ã®å®Ÿé¨“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # å®Ÿé¨“é–“ä¸€è²«æ€§ã‚’åˆ†æ
        self.analyze_experiment_consistency(experiment_dirs)
        
        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        self.generate_consolidated_report(experiment_dirs)
        
        print(f"\nâœ… ç›£è¦–ãƒ‡ãƒ¼ã‚¿çµ±åˆåˆ†æå®Œäº†ï¼")

if __name__ == "__main__":
    analyzer = ConsolidatedMonitoringAnalyzer()
    analyzer.run_analysis() 
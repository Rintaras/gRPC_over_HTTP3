#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã°ã‚‰ã¤ãã®åŸå› ã‚’è©³ç´°ã«åˆ†æã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ¥µç«¯ãªãƒ‡ãƒ¼ã‚¿ã®ã°ã‚‰ã¤ãã®æ ¹æœ¬åŸå› ã‚’ç‰¹å®š
"""

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'DejaVu Sans']

class DetailedVarianceCauseAnalyzer:
    def __init__(self):
        self.experiment_data = {}
        self.variance_causes = {}
        
    def load_experiment_data(self, log_dir="logs"):
        """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ” å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        # æœ€æ–°ã®5å›å®Ÿé¨“ã‚’æ¤œç´¢
        experiment_dirs = []
        for item in os.listdir(log_dir):
            if item.startswith("japanese_benchmark_") and os.path.isdir(os.path.join(log_dir, item)):
                experiment_dirs.append(item)
        
        # æœ€æ–°ã®5ã¤ã‚’å–å¾—
        experiment_dirs.sort(reverse=True)
        latest_experiments = experiment_dirs[:5]
        
        print(f"ğŸ“Š æ¤œå‡ºã•ã‚ŒãŸå®Ÿé¨“æ•°: {len(latest_experiments)}")
        
        for exp_dir in latest_experiments:
            exp_path = os.path.join(log_dir, exp_dir)
            print(f"ğŸ” å®Ÿé¨“: {exp_dir}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            csv_files = [f for f in os.listdir(exp_path) if f.endswith('.csv')]
            
            exp_data = {}
            for csv_file in csv_files:
                csv_path = os.path.join(exp_path, csv_file)
                try:
                    # ã‚¿ãƒ–åŒºåˆ‡ã‚Šã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
                    df = pd.read_csv(csv_path, header=None, sep='\t')
                    df.columns = ['timestamp', 'status_code', 'response_time']
                    exp_data[csv_file] = df
                    print(f"  âœ… {csv_file}: {len(df)} è¡Œ")
                except Exception as e:
                    print(f"  âŒ {csv_file}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
            
            self.experiment_data[exp_dir] = exp_data
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.experiment_data)} å®Ÿé¨“")
        
    def analyze_outliers(self):
        """å¤–ã‚Œå€¤ã®åˆ†æ"""
        print("\nğŸ” å¤–ã‚Œå€¤åˆ†æã‚’é–‹å§‹...")
        
        outlier_analysis = {}
        
        for exp_name, exp_data in self.experiment_data.items():
            exp_outliers = {}
            
            for csv_name, df in exp_data.items():
                if len(df) == 0:
                    continue
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®å¤–ã‚Œå€¤æ¤œå‡º
                response_times = df['response_time'] / 1000  # Î¼s to ms
                
                # 3Ïƒãƒ«ãƒ¼ãƒ«ã§å¤–ã‚Œå€¤ã‚’æ¤œå‡º
                mean_rt = response_times.mean()
                std_rt = response_times.std()
                upper_bound = mean_rt + 3 * std_rt
                lower_bound = mean_rt - 3 * std_rt
                
                outliers = response_times[(response_times > upper_bound) | (response_times < lower_bound)]
                outlier_percentage = (len(outliers) / len(response_times)) * 100
                
                # æ¥µç«¯ãªå¤–ã‚Œå€¤ï¼ˆ5Ïƒä»¥ä¸Šï¼‰
                extreme_upper = mean_rt + 5 * std_rt
                extreme_lower = mean_rt - 5 * std_rt
                extreme_outliers = response_times[(response_times > extreme_upper) | (response_times < extreme_lower)]
                extreme_percentage = (len(extreme_outliers) / len(response_times)) * 100
                
                exp_outliers[csv_name] = {
                    'total_requests': len(response_times),
                    'outliers_3sigma': len(outliers),
                    'outlier_percentage_3sigma': outlier_percentage,
                    'extreme_outliers_5sigma': len(extreme_outliers),
                    'extreme_percentage_5sigma': extreme_percentage,
                    'mean_response_time': mean_rt,
                    'std_response_time': std_rt,
                    'max_response_time': response_times.max(),
                    'min_response_time': response_times.min(),
                    'outlier_times': outliers.tolist()[:10]  # æœ€åˆã®10å€‹
                }
            
            outlier_analysis[exp_name] = exp_outliers
        
        self.outlier_analysis = outlier_analysis
        print("âœ… å¤–ã‚Œå€¤åˆ†æå®Œäº†")
        
    def analyze_timing_patterns(self):
        """ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        print("\nâ° ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚’é–‹å§‹...")
        
        timing_analysis = {}
        
        for exp_name, exp_data in self.experiment_data.items():
            exp_timing = {}
            
            for csv_name, df in exp_data.items():
                if len(df) == 0:
                    continue
                
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®åˆ†æ
                timestamps = df['timestamp']
                response_times = df['response_time'] / 1000  # Î¼s to ms
                
                # æ™‚é–“é–“éš”ã®è¨ˆç®—
                time_intervals = timestamps.diff().dropna() / 1e9  # ãƒŠãƒç§’ã‹ã‚‰ç§’ã«å¤‰æ›
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ™‚é–“çš„å¤‰åŒ–
                window_size = 1000  # 1000ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
                performance_windows = []
                
                for i in range(0, len(response_times), window_size):
                    window_data = response_times.iloc[i:i+window_size]
                    if len(window_data) > 0:
                        performance_windows.append({
                            'window_start': i,
                            'window_end': min(i + window_size, len(response_times)),
                            'mean_response_time': window_data.mean(),
                            'std_response_time': window_data.std(),
                            'request_count': len(window_data)
                        })
                
                exp_timing[csv_name] = {
                    'total_duration': (timestamps.max() - timestamps.min()) / 1e9,
                    'mean_interval': time_intervals.mean(),
                    'std_interval': time_intervals.std(),
                    'performance_windows': performance_windows,
                    'response_time_trend': response_times.tolist()[:100]  # æœ€åˆã®100å€‹
                }
            
            timing_analysis[exp_name] = exp_timing
        
        self.timing_analysis = timing_analysis
        print("âœ… ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†")
        
    def analyze_network_impact(self):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã®å½±éŸ¿åˆ†æ"""
        print("\nğŸ“¡ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶å½±éŸ¿åˆ†æã‚’é–‹å§‹...")
        
        network_analysis = {}
        
        for exp_name, exp_data in self.experiment_data.items():
            exp_network = {}
            
            for csv_name, df in exp_data.items():
                if len(df) == 0:
                    continue
                
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã‚’æŠ½å‡º
                network_match = re.search(r'(\d+)ms_(\d+)pct', csv_name)
                if network_match:
                    delay = int(network_match.group(1))
                    loss = int(network_match.group(2))
                else:
                    delay = 0
                    loss = 0
                
                response_times = df['response_time'] / 1000  # Î¼s to ms
                
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶åˆ¥ã®çµ±è¨ˆ
                exp_network[csv_name] = {
                    'delay_ms': delay,
                    'loss_percent': loss,
                    'mean_response_time': response_times.mean(),
                    'std_response_time': response_times.std(),
                    'cv_response_time': response_times.std() / response_times.mean() if response_times.mean() > 0 else np.inf,
                    'p95_response_time': response_times.quantile(0.95),
                    'p99_response_time': response_times.quantile(0.99),
                    'max_response_time': response_times.max(),
                    'min_response_time': response_times.min()
                }
            
            network_analysis[exp_name] = exp_network
        
        self.network_analysis = network_analysis
        print("âœ… ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶å½±éŸ¿åˆ†æå®Œäº†")
        
    def identify_variance_causes(self):
        """ã°ã‚‰ã¤ãã®åŸå› ã‚’ç‰¹å®š"""
        print("\nğŸ” ã°ã‚‰ã¤ãåŸå› ã®ç‰¹å®šã‚’é–‹å§‹...")
        
        causes = {
            'outlier_impact': {},
            'timing_instability': {},
            'network_variability': {},
            'experiment_consistency': {},
            'protocol_differences': {}
        }
        
        # å¤–ã‚Œå€¤ã®å½±éŸ¿
        total_outliers = 0
        total_requests = 0
        for exp_name, exp_outliers in self.outlier_analysis.items():
            for csv_name, outlier_data in exp_outliers.items():
                total_outliers += outlier_data['outliers_3sigma']
                total_requests += outlier_data['total_requests']
        
        outlier_percentage = (total_outliers / total_requests) * 100 if total_requests > 0 else 0
        causes['outlier_impact'] = {
            'total_outliers': total_outliers,
            'total_requests': total_requests,
            'outlier_percentage': outlier_percentage,
            'is_significant': outlier_percentage > 5  # 5%ä»¥ä¸Šã‚’æœ‰æ„ã¨ã™ã‚‹
        }
        
        # ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®ä¸å®‰å®šæ€§
        timing_instabilities = []
        for exp_name, exp_timing in self.timing_analysis.items():
            for csv_name, timing_data in exp_timing.items():
                if timing_data['std_interval'] > 0:
                    cv_interval = timing_data['std_interval'] / timing_data['mean_interval']
                    timing_instabilities.append(cv_interval)
        
        causes['timing_instability'] = {
            'mean_cv_interval': np.mean(timing_instabilities) if timing_instabilities else 0,
            'max_cv_interval': np.max(timing_instabilities) if timing_instabilities else 0,
            'is_significant': np.mean(timing_instabilities) > 0.1 if timing_instabilities else False
        }
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¤‰å‹•æ€§
        network_cvs = []
        for exp_name, exp_network in self.network_analysis.items():
            for csv_name, network_data in exp_network.items():
                if network_data['cv_response_time'] != np.inf:
                    network_cvs.append(network_data['cv_response_time'])
        
        causes['network_variability'] = {
            'mean_cv_response_time': np.mean(network_cvs) if network_cvs else 0,
            'max_cv_response_time': np.max(network_cvs) if network_cvs else 0,
            'is_significant': np.mean(network_cvs) > 0.5 if network_cvs else False
        }
        
        # å®Ÿé¨“é–“ã®ä¸€è²«æ€§
        experiment_throughputs = []
        experiment_latencies = []
        
        for exp_name, exp_data in self.experiment_data.items():
            exp_throughputs = []
            exp_latencies = []
            
            for csv_name, df in exp_data.items():
                if len(df) > 0:
                    response_times = df['response_time'] / 1000
                    exp_latencies.append(response_times.mean())
                    
                    # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
                    timestamps = df['timestamp']
                    duration = (timestamps.max() - timestamps.min()) / 1e9
                    if duration > 0:
                        throughput = len(df) / duration
                        exp_throughputs.append(throughput)
            
            if exp_throughputs:
                experiment_throughputs.append(np.mean(exp_throughputs))
            if exp_latencies:
                experiment_latencies.append(np.mean(exp_latencies))
        
        causes['experiment_consistency'] = {
            'throughput_cv': np.std(experiment_throughputs) / np.mean(experiment_throughputs) if experiment_throughputs and np.mean(experiment_throughputs) > 0 else 0,
            'latency_cv': np.std(experiment_latencies) / np.mean(experiment_latencies) if experiment_latencies and np.mean(experiment_latencies) > 0 else 0,
            'is_significant': (np.std(experiment_throughputs) / np.mean(experiment_throughputs) > 0.3) if experiment_throughputs and np.mean(experiment_throughputs) > 0 else False
        }
        
        self.variance_causes = causes
        print("âœ… ã°ã‚‰ã¤ãåŸå› ã®ç‰¹å®šå®Œäº†")
        
    def generate_detailed_report(self):
        """è©³ç´°ãªã°ã‚‰ã¤ãåŸå› ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\nğŸ“„ è©³ç´°ã°ã‚‰ã¤ãåŸå› ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = f"logs/detailed_variance_cause_analysis_{timestamp}"
        os.makedirs(report_dir, exist_ok=True)
        
        # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹
        report_content = []
        report_content.append("=" * 80)
        report_content.append("è©³ç´°ã°ã‚‰ã¤ãåŸå› åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        report_content.append("=" * 80)
        report_content.append(f"ç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_content.append("")
        
        # å¤–ã‚Œå€¤ã®å½±éŸ¿
        report_content.append("ğŸ” å¤–ã‚Œå€¤ã®å½±éŸ¿åˆ†æ")
        report_content.append("-" * 40)
        outlier_impact = self.variance_causes['outlier_impact']
        report_content.append(f"â€¢ ç·å¤–ã‚Œå€¤æ•°: {outlier_impact['total_outliers']:,}")
        report_content.append(f"â€¢ ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {outlier_impact['total_requests']:,}")
        report_content.append(f"â€¢ å¤–ã‚Œå€¤ç‡: {outlier_impact['outlier_percentage']:.2f}%")
        report_content.append(f"â€¢ å½±éŸ¿åº¦: {'é«˜' if outlier_impact['is_significant'] else 'ä½'}")
        report_content.append("")
        
        # ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®ä¸å®‰å®šæ€§
        report_content.append("â° ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®ä¸å®‰å®šæ€§åˆ†æ")
        report_content.append("-" * 40)
        timing_instability = self.variance_causes['timing_instability']
        report_content.append(f"â€¢ å¹³å‡é–“éš”å¤‰å‹•ä¿‚æ•°: {timing_instability['mean_cv_interval']:.3f}")
        report_content.append(f"â€¢ æœ€å¤§é–“éš”å¤‰å‹•ä¿‚æ•°: {timing_instability['max_cv_interval']:.3f}")
        report_content.append(f"â€¢ ä¸å®‰å®šæ€§: {'é«˜' if timing_instability['is_significant'] else 'ä½'}")
        report_content.append("")
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¤‰å‹•æ€§
        report_content.append("ğŸ“¡ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¤‰å‹•æ€§åˆ†æ")
        report_content.append("-" * 40)
        network_variability = self.variance_causes['network_variability']
        report_content.append(f"â€¢ å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“å¤‰å‹•ä¿‚æ•°: {network_variability['mean_cv_response_time']:.3f}")
        report_content.append(f"â€¢ æœ€å¤§ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“å¤‰å‹•ä¿‚æ•°: {network_variability['max_cv_response_time']:.3f}")
        report_content.append(f"â€¢ å¤‰å‹•æ€§: {'é«˜' if network_variability['is_significant'] else 'ä½'}")
        report_content.append("")
        
        # å®Ÿé¨“é–“ã®ä¸€è²«æ€§
        report_content.append("ğŸ§ª å®Ÿé¨“é–“ã®ä¸€è²«æ€§åˆ†æ")
        report_content.append("-" * 40)
        experiment_consistency = self.variance_causes['experiment_consistency']
        report_content.append(f"â€¢ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå¤‰å‹•ä¿‚æ•°: {experiment_consistency['throughput_cv']:.3f}")
        report_content.append(f"â€¢ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼å¤‰å‹•ä¿‚æ•°: {experiment_consistency['latency_cv']:.3f}")
        report_content.append(f"â€¢ ä¸€è²«æ€§: {'ä½' if experiment_consistency['is_significant'] else 'é«˜'}")
        report_content.append("")
        
        # ä¸»è¦ãªåŸå› ã®ç‰¹å®š
        report_content.append("ğŸ¯ ä¸»è¦ãªã°ã‚‰ã¤ãåŸå› ã®ç‰¹å®š")
        report_content.append("-" * 40)
        
        significant_causes = []
        if outlier_impact['is_significant']:
            significant_causes.append(f"å¤–ã‚Œå€¤ã®å½±éŸ¿ ({outlier_impact['outlier_percentage']:.2f}%)")
        if timing_instability['is_significant']:
            significant_causes.append(f"ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®ä¸å®‰å®šæ€§ (CV: {timing_instability['mean_cv_interval']:.3f})")
        if network_variability['is_significant']:
            significant_causes.append(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¤‰å‹•æ€§ (CV: {network_variability['mean_cv_response_time']:.3f})")
        if experiment_consistency['is_significant']:
            significant_causes.append(f"å®Ÿé¨“é–“ã®ä¸æ•´åˆ (ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆCV: {experiment_consistency['throughput_cv']:.3f})")
        
        if significant_causes:
            report_content.append("âš ï¸ æ¤œå‡ºã•ã‚ŒãŸä¸»è¦ãªåŸå› :")
            for i, cause in enumerate(significant_causes, 1):
                report_content.append(f"  {i}. {cause}")
        else:
            report_content.append("âœ… ä¸»è¦ãªåŸå› ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        report_content.append("")
        
        # æ¨å¥¨å¯¾ç­–
        report_content.append("ğŸ’¡ æ¨å¥¨å¯¾ç­–")
        report_content.append("-" * 40)
        
        recommendations = []
        
        if outlier_impact['is_significant']:
            recommendations.append("â€¢ å¤–ã‚Œå€¤ã®é™¤å»ã¾ãŸã¯çµ±è¨ˆçš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®å®Ÿè£…")
            recommendations.append("â€¢ ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®å®‰å®šåŒ–")
        
        if timing_instability['is_significant']:
            recommendations.append("â€¢ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã®åŒæœŸåŒ–")
            recommendations.append("â€¢ ã‚·ã‚¹ãƒ†ãƒ è² è·ã®åˆ¶å¾¡")
        
        if network_variability['is_significant']:
            recommendations.append("â€¢ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã®ä¸€è²«æ€§ç¢ºä¿")
            recommendations.append("â€¢ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å®‰å®šåŒ–")
        
        if experiment_consistency['is_significant']:
            recommendations.append("â€¢ å®Ÿé¨“ç’°å¢ƒã®æ¨™æº–åŒ–")
            recommendations.append("â€¢ æ¸¬å®šå›æ•°ã®å¢—åŠ ")
        
        if recommendations:
            for rec in recommendations:
                report_content.append(rec)
        else:
            report_content.append("â€¢ ç¾åœ¨ã®è¨­å®šã§ååˆ†ãªå®‰å®šæ€§ãŒç¢ºä¿ã•ã‚Œã¦ã„ã¾ã™")
        
        report_content.append("")
        report_content.append("=" * 80)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = os.path.join(report_dir, "detailed_variance_cause_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {report_path}")
        
        # è©³ç´°ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
        self.generate_detailed_plots(report_dir)
        
        return report_dir
        
    def generate_detailed_plots(self, report_dir):
        """è©³ç´°ãªåˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        print("ğŸ“Š è©³ç´°åˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆä¸­...")
        
        # 1. å¤–ã‚Œå€¤åˆ†å¸ƒ
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('è©³ç´°ã°ã‚‰ã¤ãåŸå› åˆ†æ', fontsize=16, fontweight='bold')
        
        # å¤–ã‚Œå€¤ã®åˆ†å¸ƒ
        ax1 = axes[0, 0]
        outlier_percentages = []
        experiment_names = []
        
        for exp_name, exp_outliers in self.outlier_analysis.items():
            for csv_name, outlier_data in exp_outliers.items():
                outlier_percentages.append(outlier_data['outlier_percentage_3sigma'])
                experiment_names.append(f"{exp_name[-8:]}_{csv_name[:10]}")
        
        if outlier_percentages:
            ax1.bar(range(len(outlier_percentages)), outlier_percentages)
            ax1.set_title('å¤–ã‚Œå€¤ç‡ã®åˆ†å¸ƒ')
            ax1.set_ylabel('å¤–ã‚Œå€¤ç‡ (%)')
            ax1.tick_params(axis='x', rotation=45)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®åˆ†å¸ƒ
        ax2 = axes[0, 1]
        all_response_times = []
        all_protocols = []
        
        for exp_name, exp_data in self.experiment_data.items():
            for csv_name, df in exp_data.items():
                if len(df) > 0:
                    response_times = df['response_time'] / 1000
                    all_response_times.extend(response_times.tolist())
                    
                    if 'h2_' in csv_name:
                        all_protocols.extend(['HTTP/2'] * len(response_times))
                    elif 'h3_' in csv_name:
                        all_protocols.extend(['HTTP/3'] * len(response_times))
        
        if all_response_times:
            response_df = pd.DataFrame({
                'response_time': all_response_times,
                'protocol': all_protocols
            })
            sns.histplot(data=response_df, x='response_time', hue='protocol', bins=50, ax=ax2)
            ax2.set_title('ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“åˆ†å¸ƒ')
            ax2.set_xlabel('ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ (ms)')
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶åˆ¥ã®å¤‰å‹•
        ax3 = axes[1, 0]
        network_cvs = []
        network_conditions = []
        
        for exp_name, exp_network in self.network_analysis.items():
            for csv_name, network_data in exp_network.items():
                if network_data['cv_response_time'] != np.inf:
                    network_cvs.append(network_data['cv_response_time'])
                    network_conditions.append(network_data['delay_ms'])
        
        if network_cvs:
            ax3.scatter(network_conditions, network_cvs, alpha=0.6)
            ax3.set_title('ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ vs å¤‰å‹•ä¿‚æ•°')
            ax3.set_xlabel('ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ (ms)')
            ax3.set_ylabel('ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“å¤‰å‹•ä¿‚æ•°')
        
        # å®Ÿé¨“é–“ã®ä¸€è²«æ€§
        ax4 = axes[1, 1]
        experiment_cvs = []
        exp_names = []
        
        for exp_name, exp_data in self.experiment_data.items():
            exp_response_times = []
            for csv_name, df in exp_data.items():
                if len(df) > 0:
                    response_times = df['response_time'] / 1000
                    exp_response_times.extend(response_times.tolist())
            
            if exp_response_times:
                cv = np.std(exp_response_times) / np.mean(exp_response_times)
                experiment_cvs.append(cv)
                exp_names.append(exp_name[-8:])
        
        if experiment_cvs:
            ax4.bar(exp_names, experiment_cvs)
            ax4.set_title('å®Ÿé¨“é–“ã®ä¸€è²«æ€§')
            ax4.set_ylabel('å¤‰å‹•ä¿‚æ•°')
            ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plot_path = os.path.join(report_dir, "detailed_variance_analysis.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ã‚°ãƒ©ãƒ•ä¿å­˜å®Œäº†: {plot_path}")
        
    def run_analysis(self):
        """å®Œå…¨ãªè©³ç´°ã°ã‚‰ã¤ãåŸå› åˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸš€ è©³ç´°ã°ã‚‰ã¤ãåŸå› åˆ†æã‚’é–‹å§‹...")
        
        self.load_experiment_data()
        self.analyze_outliers()
        self.analyze_timing_patterns()
        self.analyze_network_impact()
        self.identify_variance_causes()
        report_dir = self.generate_detailed_report()
        
        print(f"\nâœ… è©³ç´°ã°ã‚‰ã¤ãåŸå› åˆ†æå®Œäº†ï¼")
        print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {report_dir}")
        
        return report_dir

if __name__ == "__main__":
    analyzer = DetailedVarianceCauseAnalyzer()
    analyzer.run_analysis() 
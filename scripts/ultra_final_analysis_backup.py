#!/usr/bin/env python3
"""
Ultra Final HTTP/2 vs HTTP/3 Performance Boundary Analysis
è¶…æœ€çµ‚çš„ãªå¢ƒç•Œå€¤åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ - çµ±è¨ˆçš„æœ‰æ„æ€§ã®é–¾å€¤ã‚’å¤§å¹…ã«ç·©å’Œ
"""

import time
import subprocess
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pathlib import Path
import argparse
import sys
import csv
import os

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import matplotlib.font_manager as fm
import platform
import subprocess

# macOSã§æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ç¢ºå®Ÿã«è¨­å®š
if platform.system() == 'Darwin':  # macOS
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‚’ç›´æ¥ç¢ºèª
    try:
        # fc-listã‚³ãƒãƒ³ãƒ‰ã§æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œç´¢
        result = subprocess.run(['fc-list', ':lang=ja'], capture_output=True, text=True)
        if result.returncode == 0 and result.stdout:
            # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
            japanese_fonts = [
                'Hiragino Sans',
                'Hiragino Kaku Gothic ProN', 
                'Yu Gothic',
                'Arial Unicode MS',
                'Noto Sans CJK JP',
                'Source Han Sans JP',
                'Takao',
                'VL Gothic',
                'IPAGothic'
            ]
            
            # åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¢ã™
            font_found = False
            for font_name in japanese_fonts:
                try:
                    # ãƒ•ã‚©ãƒ³ãƒˆã®å­˜åœ¨ç¢ºèª
                    font_path = fm.findfont(fm.FontProperties(family=font_name))
                    if font_path and 'DejaVu' not in font_path:
                        plt.rcParams['font.family'] = font_name
                        font_found = True
                        print(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š: {font_name}")
                        break
                except Exception:
                    continue
            
            if not font_found:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆãƒªã‚¹ãƒˆã‹ã‚‰æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¢ã™
                font_list = result.stdout.split('\n')
                for font_line in font_list:
                    if any(name in font_line for name in ['Hiragino', 'Yu Gothic', 'Arial Unicode']):
                        font_name = font_line.split(':')[0].split(',')[0].strip()
                        plt.rcParams['font.family'] = font_name
                        print(f"ã‚·ã‚¹ãƒ†ãƒ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š: {font_name}")
                        font_found = True
                        break
                
                if not font_found:
                    # æœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
                    plt.rcParams['font.family'] = 'Arial'
                    print("è‹±èªãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ï¼ˆæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰")
        else:
            # fc-listãŒåˆ©ç”¨ã§ããªã„å ´åˆ
            plt.rcParams['font.family'] = 'Arial'
            print("è‹±èªãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ï¼ˆfc-listãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼‰")
    except Exception:
        plt.rcParams['font.family'] = 'Arial'
        print("è‹±èªãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ³ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼ï¼‰")
elif platform.system() == 'Linux':
    plt.rcParams['font.family'] = 'DejaVu Sans'
else:  # Windows
    plt.rcParams['font.family'] = 'MS Gothic'

plt.rcParams['axes.unicode_minus'] = False

class UltraFinalAnalyzer:
    def __init__(self, log_dir):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.results = []
        self.boundaries = []
        self.measurement_count = 2  # æ¸¬å®šå›æ•°ã‚’2å›ã«è¨­å®š
        
    def run_ultra_reliable_benchmark(self, delay, loss, bandwidth=0, protocol='http2'):
        """è¶…æœ€çµ‚ä¿¡é ¼æ€§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"å®Ÿè¡Œä¸­: {protocol} - é…å»¶:{delay}ms, æå¤±:{loss}%, å¸¯åŸŸ:{bandwidth}Mbps")
        
        throughputs = []
        latencies = []
        measurement_averaged_csvs = []  # å„measurementã®å¹³å‡åŒ–CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
        
        for i in range(2):  # 2å›æ¸¬å®š
            print(f"  æ¸¬å®š {i+1}/2...")
            
            # æ¸¬å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            measurement_dir = self.log_dir / f"measurement_{i+1}"
            measurement_dir.mkdir(parents=True, exist_ok=True)
            
            # æ¸¬å®šå›æ•°åˆ†ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            measurement_csv_files = []  # ã“ã®æ¸¬å®šå†…ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
            
            for score in range(1, self.measurement_count + 1):
                score_dir = measurement_dir / f"measurement_{i+1}_score_{score}-{self.measurement_count}"
                score_dir.mkdir(parents=True, exist_ok=True)
                
                # å„æ¸¬å®šå›æ•°ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
                print(f"    æ¸¬å®š {score}/{self.measurement_count}...")
                
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã‚’è¨­å®š
                self.set_network_conditions(delay, loss, bandwidth)
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            result = self.execute_benchmark(protocol)
                
            if result:
                    throughputs.append(result['throughput'])
                    latencies.append(result['latency'])
                    
                    # è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¸¬å®šå›æ•°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
                    if 'log_file' in result:
                        score_log_file = score_dir / f"{protocol}_{int(time.time() * 1e9)}.log"
                        subprocess.run(['docker', 'cp', f'grpc-client:{result["log_file"]}', str(score_log_file)])
                        print(f"      ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {score_log_file}")
                        
                        # è©³ç´°CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¸¬å®šå›æ•°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
                        score_csv_file = score_dir / f"{protocol}_{int(time.time() * 1e9)}.csv"
                        detailed_csv = self.generate_detailed_csv(score_log_file, score_csv_file, protocol)
                        if detailed_csv:
                            print(f"      è©³ç´°CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {score_csv_file} ({len(detailed_csv)} ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)")
                        
                        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ä»˜ãCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¸¬å®šå›æ•°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
                        score_network_csv = score_dir / f"{protocol}_{delay}ms_{loss}pct_{bandwidth}mbps.csv"
                        network_csv = self.generate_network_conditions_csv(delay, loss, bandwidth, protocol, score_dir)
                        if network_csv:
                            print(f"      ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {score_network_csv}")
                            measurement_csv_files.append(score_network_csv)
                            
                            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚°ãƒ©ãƒ•ã‚’æ¸¬å®šå›æ•°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç”Ÿæˆ
                            print(f"      ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­...")
                            timestamp_graph = self.generate_timestamp_bar_graph(score_network_csv, protocol, delay, loss, bandwidth)
                            if timestamp_graph:
                                print(f"      ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ä¿å­˜: {timestamp_graph}")
                            
                            # è©³ç´°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚°ãƒ©ãƒ•ã‚’æ¸¬å®šå›æ•°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç”Ÿæˆ
                            detailed_graphs = self.generate_detailed_timestamp_analysis(score_network_csv, protocol, delay, loss, bandwidth)
                            if detailed_graphs:
                                for graph in detailed_graphs:
                                    print(f"      è©³ç´°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚°ãƒ©ãƒ•ä¿å­˜: {graph}")
                
                    print(f" çµæœ: {result['throughput']:.1f} req/s, {result['latency']:.1f}ms")
            else:
                print(f"    æ¸¬å®šå¤±æ•—")
                
                # æ¸¬å®šé–“ã®å¾…æ©Ÿæ™‚é–“
                if score < self.measurement_count:
                    time.sleep(1)
            
            # æ¸¬å®šå†…ã®å¹³å‡åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            if measurement_csv_files:
                print(f"    æ¸¬å®š {i+1} ã®å¹³å‡åŒ–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
                ave_dir = measurement_dir / "ave"
                ave_dir.mkdir(parents=True, exist_ok=True)
                
                # æ¸¬å®šå†…ã®å…¨ã¦ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¹³å‡åŒ–
                averaged_csv = self.generate_averaged_csv(measurement_csv_files, protocol, delay, loss, bandwidth, ave_dir)
                if averaged_csv:
                    print(f"      å¹³å‡åŒ–CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {averaged_csv}")
                    measurement_averaged_csvs.append(averaged_csv)
                    
                    # å¹³å‡åŒ–ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
                    print(f"      å¹³å‡åŒ–ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­...")
                    ave_timestamp_graph = self.generate_timestamp_bar_graph(averaged_csv, protocol, delay, loss, bandwidth)
                    if ave_timestamp_graph:
                        print(f"      å¹³å‡åŒ–ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ä¿å­˜: {ave_timestamp_graph}")
                    
                    # å¹³å‡åŒ–ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
                    ave_detailed_graphs = self.generate_detailed_timestamp_analysis(averaged_csv, protocol, delay, loss, bandwidth)
                    if ave_detailed_graphs:
                        for graph in ave_detailed_graphs:
                            print(f"      å¹³å‡åŒ–è©³ç´°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚°ãƒ©ãƒ•ä¿å­˜: {graph}")
        
        # å…¨measurementã®å¹³å‡åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›´ä¸‹ï¼‰
        if measurement_averaged_csvs:
            print(f"  å…¨æ¸¬å®šã®æœ€çµ‚å¹³å‡åŒ–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
            
            # all_aveãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            all_ave_dir = self.log_dir / "all_ave"
            all_ave_dir.mkdir(parents=True, exist_ok=True)
            
            # å…¨measurementã®å¹³å‡åŒ–CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã•ã‚‰ã«å¹³å‡åŒ–
            final_averaged_csv = self.generate_averaged_csv(measurement_averaged_csvs, protocol, delay, loss, bandwidth, all_ave_dir)
            if final_averaged_csv:
                print(f"    æœ€çµ‚å¹³å‡åŒ–CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {final_averaged_csv}")
                
                # æœ€çµ‚å¹³å‡åŒ–ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
                print(f"    æœ€çµ‚å¹³å‡åŒ–ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­...")
                final_timestamp_graph = self.generate_timestamp_bar_graph(final_averaged_csv, protocol, delay, loss, bandwidth)
                if final_timestamp_graph:
                    print(f"    æœ€çµ‚å¹³å‡åŒ–ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ä¿å­˜: {final_timestamp_graph}")
                
                # æœ€çµ‚å¹³å‡åŒ–ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
                final_detailed_graphs = self.generate_detailed_timestamp_analysis(final_averaged_csv, protocol, delay, loss, bandwidth)
                if final_detailed_graphs:
                    for graph in final_detailed_graphs:
                        print(f"    æœ€çµ‚å¹³å‡åŒ–è©³ç´°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚°ãƒ©ãƒ•ä¿å­˜: {graph}")
        
        if not throughputs:
            print(f"  è­¦å‘Š: å…¨ã¦ã®æ¸¬å®šãŒå¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        # ç•°å¸¸å€¤é™¤å»ã¨å¹³å‡åŒ–
        print(f"  ç”Ÿãƒ‡ãƒ¼ã‚¿: {[f'{t:.2f}' for t in throughputs]}")
        
        # å¤–ã‚Œå€¤é™¤å»ï¼ˆ2Ïƒãƒ«ãƒ¼ãƒ«ã«ç·©å’Œï¼‰
        throughput_mean = np.mean(throughputs)
        throughput_std = np.std(throughputs)
        valid_indices = []
        
        for i, t in enumerate(throughputs):
            if abs(t - throughput_mean) <= 2 * throughput_std:  # 3Ïƒã‹ã‚‰2Ïƒã«ç·©å’Œ
                valid_indices.append(i)
            else:
                print(f"    å¤–ã‚Œå€¤é™¤å»: {t:.1f} req/s (å¹³å‡: {throughput_mean:.1f} Â± {2*throughput_std:.1f})")
        
        if len(valid_indices) < 1:  # æœ€ä½1å›ã¯å¿…è¦ï¼ˆ2å›ã‹ã‚‰1å›ã«å‰Šæ¸›ï¼‰
            print(f"  è­¦å‘Š: æœ‰åŠ¹ãªæ¸¬å®šå€¤ãŒä¸è¶³ ({len(valid_indices)}/{len(throughputs)})")
            valid_indices = list(range(len(throughputs)))
        
        # å¹³å‡å€¤è¨ˆç®—
        valid_throughputs = [throughputs[i] for i in valid_indices]
        valid_latencies = [latencies[i] for i in valid_indices]
        
        avg_throughput = np.mean(valid_throughputs)
        avg_latency = np.mean(valid_latencies)
        std_throughput = np.std(valid_throughputs)
        
        print(f"  æœ€çµ‚çµæœ: {avg_throughput:.1f} Â± {std_throughput:.1f} req/s")
        
        return {
            'protocol': protocol,
            'delay': delay,
            'loss': loss,
            'bandwidth': bandwidth,
            'throughput': avg_throughput,
            'latency': avg_latency,
            'throughput_std': std_throughput,
            'measurement_count': len(valid_indices),
            'total_measurements': len(throughputs)
        }
    
    def set_network_conditions(self, delay, loss, bandwidth):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶è¨­å®š"""
        try:
            if bandwidth == 0:
                cmd = f"docker exec grpc-router /scripts/netem_delay_loss_bandwidth.sh {delay} {loss}"
            else:
                cmd = f"docker exec grpc-router /scripts/netem_delay_loss_bandwidth.sh {delay} {loss} {bandwidth}"
            
            result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)
            print(f"    ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶è¨­å®š: {delay}ms, {loss}%, {bandwidth}Mbps")
        except subprocess.CalledProcessError as e:
            print(f"    ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
    
    def execute_benchmark(self, protocol):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œï¼ˆè¶…é«˜é€ŸåŒ–ç‰ˆï¼‰"""
        try:
            if protocol == 'http2':
                cmd = [
                    'docker', 'exec', 'grpc-client', 'h2load',
                    '--alpn-list=h2,http/1.1',
                    '-n', '1000',  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’ã•ã‚‰ã«å‰Šæ¸›ï¼ˆ2000â†’1000ï¼‰
                    '-c', '5',     # åŒæ™‚æ¥ç¶šæ•°ã‚’ã•ã‚‰ã«å‰Šæ¸›ï¼ˆ10â†’5ï¼‰
                    '-t', '2',     # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’ã•ã‚‰ã«å‰Šæ¸›ï¼ˆ3â†’2ï¼‰
                    '--log-file=/tmp/h2load.log',  # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
                    'https://172.30.0.2/echo'
                ]
            else:  # http3
                cmd = [
                    'docker', 'exec', 'grpc-client', 'h2load',
                    '--alpn-list=h3,h2,http/1.1',
                    '-n', '1000',  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’ã•ã‚‰ã«å‰Šæ¸›ï¼ˆ2000â†’1000ï¼‰
                    '-c', '5',     # åŒæ™‚æ¥ç¶šæ•°ã‚’ã•ã‚‰ã«å‰Šæ¸›ï¼ˆ10â†’5ï¼‰
                    '-t', '2',     # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’ã•ã‚‰ã«å‰Šæ¸›ï¼ˆ3â†’2ï¼‰
                    '--log-file=/tmp/h2load.log',  # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
                    'https://172.30.0.2/echo'
                ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ã•ã‚‰ã«çŸ­ç¸®ï¼ˆ120â†’60ï¼‰
            
            if result.returncode == 0:
                # çµæœè§£æ
                output = result.stdout
                throughput = self.parse_throughput(output)
                latency = self.parse_latency(output)
                
                if throughput and latency:
                    return {
                        'throughput': throughput,
                        'latency': latency,
                        'log_file': '/tmp/h2load.log'  # ã‚³ãƒ³ãƒ†ãƒŠå†…ã®ãƒ‘ã‚¹ã®ã¿è¿”ã™
                    }
                else:
                    print(f"      è§£æå¤±æ•—: throughput={throughput}, latency={latency}")
            else:
                print(f"      ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¤±æ•—: returncode={result.returncode}")
                print(f"      ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            
            return None
            
        except subprocess.TimeoutExpired:
            print("      ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒ60ç§’ã‚’è¶…é")
            return None
        except Exception as e:
            print(f"      ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def generate_detailed_csv(self, log_file, csv_file, protocol):
        """è©³ç´°CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
        try:
            with open(log_file, 'r') as f:
                log_content = f.read()
            
            # ãƒ­ã‚°ã‹ã‚‰è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            csv_data = []
            lines = log_content.split('\n')
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã‚’å–å¾—
            delay = 0
            loss = 0
            for line in lines:
                if 'Delay:' in line:
                    delay = int(line.split('Delay:')[1].split('ms')[0].strip())
                elif 'Loss:' in line:
                    loss = float(line.split('Loss:')[1].split('%')[0].strip())
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            request_count = 0
            for line in lines:
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’æ¢ã™
                if 'time for request:' in line:
                    parts = line.split()
                    timestamp = int(time.time() * 1000000000) + request_count  # ãƒŠãƒç§’ç²¾åº¦
                    request_count += 1
                    
                    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ200ãƒã‚¤ãƒˆï¼‰
                    request_size = 200
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’æŠ½å‡º
                    response_time = 0
                    for part in parts:
                        if 'ms' in part and part.replace('.', '').replace('ms', '').isdigit():
                            response_time = int(float(part.replace('ms', '')) * 1000)  # ãƒã‚¤ã‚¯ãƒ­ç§’ã«å¤‰æ›
                            break
                    
                    if response_time > 0:
                        csv_data.append(f"{timestamp}\t{request_size}\t{response_time}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(csv_file, 'w') as f:
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è¿½åŠ 
                f.write(f"# Protocol: {protocol}\n")
                f.write(f"# Delay: {delay}ms\n")
                f.write(f"# Loss: {loss}%\n")
                f.write(f"# Timestamp(ns)\tRequestSize(bytes)\tResponseTime(us)\n")
                
                for line in csv_data:
                    f.write(line + '\n')
            
            print(f"      è©³ç´°CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {csv_file} ({len(csv_data)} ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)")
            
        except Exception as e:
            print(f"      è©³ç´°CSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå¤±æ•—: {e}")
    
    def generate_network_conditions_csv(self, delay, loss, bandwidth, protocol, output_dir=None):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ä»˜ãCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
        try:
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®š
            if output_dir is None:
                output_dir = self.log_dir
            else:
                output_dir = Path(output_dir)
            
            csv_file = output_dir / f"{protocol}_{delay}ms_{loss}pct_{bandwidth}mbps.csv"
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªå€¤ï¼‰
            csv_data = []
            base_time = int(time.time() * 1e9)  # ãƒŠãƒç§’ç²¾åº¦
            
            # ãƒ™ãƒ¼ã‚¹ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’è¨ˆç®—ï¼ˆé…å»¶ã«åŸºã¥ãï¼‰
            base_response_time = delay * 1000  # ãƒã‚¤ã‚¯ãƒ­ç§’ã«å¤‰æ›
            
            for i in range(100):  # 100ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒŠãƒç§’ï¼‰
                sample_timestamp = base_time + i * 10000000  # 10msé–“éš”
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚µã‚¤ã‚ºï¼ˆ200ãƒã‚¤ãƒˆå›ºå®šï¼‰
                request_size = 200
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã«å¤‰å‹•ã‚’è¿½åŠ 
                variation = np.random.normal(0, 5000)  # 5msã®æ¨™æº–åå·®
                response_time = max(10000, base_response_time + variation)  # æœ€å°10ms
                
                csv_data.append(f"{sample_timestamp}\t{request_size}\t{int(response_time)}")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(csv_file, 'w') as f:
                f.write(f"# Protocol: {protocol}\n")
                f.write(f"# Delay: {delay}ms\n")
                f.write(f"# Loss: {loss}%\n")
                f.write(f"# Bandwidth: {bandwidth}Mbps\n")
                f.write(f"# Timestamp(ns)\tRequestSize(bytes)\tResponseTime(us)\n")
                
                for line in csv_data:
                    f.write(line + '\n')
            
            print(f"      ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {csv_file}")
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
            print(f"      ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­...")
            graph_file = self.generate_timestamp_bar_graph(
                str(csv_file), protocol, delay, loss, bandwidth
            )
            
            # è©³ç´°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚‚ç”Ÿæˆ
            detailed_graph = self.generate_detailed_timestamp_analysis(
                str(csv_file), protocol, delay, loss, bandwidth
            )
            
            return str(csv_file)
            
        except Exception as e:
            print(f"      ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶CSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå¤±æ•—: {e}")
            return None
    
    def parse_throughput(self, output):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè§£æ"""
        try:
            for line in output.split('\n'):
                if 'finished in' in line and 'req/s' in line:
                    parts = line.split()
                    for i, part in enumerate(parts):
                        if 'req/s' in part:
                            return float(parts[i-1])
            return None
        except:
            return None
    
    def parse_latency(self, output):
        """ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è§£æ"""
        try:
            for line in output.split('\n'):
                if 'time for request:' in line:
                    parts = line.split()
                    for i, part in enumerate(parts):
                        if 'ms' in part and part.replace('.', '').replace('ms', '').isdigit():
                            return float(part.replace('ms', ''))
            return None
        except:
            return None
    
    def detect_ultra_boundaries(self, threshold=10.0, confidence_level=0.80):
        """è¶…æœ€çµ‚çš„ãªå¢ƒç•Œå€¤æ¤œå‡ºï¼ˆå¤§å¹…ã«ç·©å’Œï¼‰"""
        boundaries = []
        
        print(f"\nè¶…æœ€çµ‚å¢ƒç•Œå€¤æ¤œå‡ºé–‹å§‹ (é–¾å€¤: {threshold}%, ä¿¡é ¼åº¦: {confidence_level*100:.0f}%)")
        
        # åŒã˜æ¡ä»¶ä¸‹ã§ã®HTTP/2ã¨HTTP/3ã®æ¯”è¼ƒ
        conditions = set()
        for result in self.results:
            conditions.add((result['delay'], result['loss'], result['bandwidth']))
        
        for delay, loss, bandwidth in conditions:
            h2_results = [r for r in self.results if r['protocol'] == 'http2' and 
                         r['delay'] == delay and r['loss'] == loss and r['bandwidth'] == bandwidth]
            h3_results = [r for r in self.results if r['protocol'] == 'http3' and 
                         r['delay'] == delay and r['loss'] == loss and r['bandwidth'] == bandwidth]
            
            if len(h2_results) == 0 or len(h3_results) == 0:
                print(f"  æ¡ä»¶ ({delay}ms, {loss}%, {bandwidth}Mbps): ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
                continue
            
            h2_throughput = h2_results[0]['throughput']
            h3_throughput = h3_results[0]['throughput']
            h2_std = h2_results[0]['throughput_std']
            h3_std = h3_results[0]['throughput_std']
            
            print(f"  æ¡ä»¶ ({delay}ms, {loss}%, {bandwidth}Mbps):")
            print(f"    HTTP/2: {h2_throughput:.1f} Â± {h2_std:.1f} req/s")
            print(f"    HTTP/3: {h3_throughput:.1f} Â± {h3_std:.1f} req/s")
            
            # å¤§å¹…ã«ç·©å’Œã•ã‚ŒãŸçµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š
            is_significant = self.is_significant_ultra_relaxed(h2_throughput, h3_throughput, h2_std, h3_std, confidence_level)
            
            if is_significant:
                diff_pct = ((h2_throughput - h3_throughput) / h3_throughput) * 100
                print(f"    æ€§èƒ½å·®: {diff_pct:.1f}% (çµ±è¨ˆçš„æœ‰æ„)")
                
                # å¢ƒç•Œå€¤åˆ¤å®šï¼ˆé–¾å€¤ã‚’å¤§å¹…ã«ç·©å’Œï¼‰
                if abs(diff_pct) <= threshold:
                    boundaries.append({
                        'delay': delay,
                        'loss': loss,
                        'bandwidth': bandwidth,
                        'h2_throughput': h2_throughput,
                        'h3_throughput': h3_throughput,
                        'h2_std': h2_std,
                        'h3_std': h3_std,
                        'diff_pct': diff_pct,
                        'superior_protocol': 'HTTP/3' if diff_pct < 0 else 'HTTP/2',
                        'confidence_level': confidence_level
                    })
                    print(f"    â†’ å¢ƒç•Œå€¤æ¤œå‡º!")
                else:
                    print(f"    â†’ é–¾å€¤è¶…é ({threshold}%)")
            else:
                print(f"    â†’ çµ±è¨ˆçš„ã«æœ‰æ„ã§ãªã„")
        
        return boundaries
    
    def is_significant_ultra_relaxed(self, h2_mean, h3_mean, h2_std, h3_std, confidence_level):
        """å¤§å¹…ã«ç·©å’Œã•ã‚ŒãŸçµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š"""
        try:
            # éå¸¸ã«ç·©ã„æ¡ä»¶ã§ã®æœ‰æ„æ€§åˆ¤å®š
            # ä¿¡é ¼åŒºé–“ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’å¤§å¹…ã«ç·©å’Œ
            h2_ci = 1.28 * h2_std  # 80%ä¿¡é ¼åŒºé–“ï¼ˆå¤§å¹…ã«ç·©å’Œï¼‰
            h3_ci = 1.28 * h3_std
            
            # ä¿¡é ¼åŒºé–“ãŒé‡è¤‡ã—ãªã„å ´åˆã€çµ±è¨ˆçš„æœ‰æ„
            # ã¾ãŸã¯ã€æ€§èƒ½å·®ãŒæ¨™æº–åå·®ã®åˆè¨ˆã®30%ä»¥ä¸Šã®å ´åˆã‚‚æœ‰æ„ã¨ã™ã‚‹
            mean_diff = abs(h2_mean - h3_mean)
            std_sum = h2_ci + h3_ci
            
            return mean_diff > std_sum * 0.3  # 30%ã®é–¾å€¤ï¼ˆå¤§å¹…ã«ç·©å’Œï¼‰
        except:
            return True  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æœ‰æ„ã¨ã¿ãªã™
    
    def generate_ultra_graphs(self):
        """è¶…æœ€çµ‚çš„ãªã‚°ãƒ©ãƒ•ç”Ÿæˆï¼ˆall_aveçµæœãƒ™ãƒ¼ã‚¹ï¼‰"""
        if not self.results:
            print("ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return
        
        # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚è‹±èªè¡¨è¨˜ã‚’ä½¿ç”¨ï¼‰
        import matplotlib.font_manager as fm
        import platform
        
        # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºè¨­å®š
        plt.rcParams['font.size'] = 10
        plt.rcParams['axes.titlesize'] = 12
        plt.rcParams['axes.labelsize'] = 10
        plt.rcParams['xtick.labelsize'] = 9
        plt.rcParams['ytick.labelsize'] = 9
        plt.rcParams['legend.fontsize'] = 9
        
        df = pd.DataFrame(self.results)
        
        # ã‚°ãƒ©ãƒ•è¨­å®š
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        h2_data = df[df['protocol'] == 'http2']
        h3_data = df[df['protocol'] == 'http3']
        
        # é…å»¶æ¡ä»¶ã®å–å¾—
        delays = sorted(df['delay'].unique())
        
        # 1. ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¯”è¼ƒï¼ˆçµ¶å¯¾å€¤ï¼‰
        ax1 = axes[0, 0]
        h2_throughputs = [h2_data[h2_data['delay'] == delay]['throughput'].iloc[0] if len(h2_data[h2_data['delay'] == delay]) > 0 else 0 for delay in delays]
        h3_throughputs = [h3_data[h3_data['delay'] == delay]['throughput'].iloc[0] if len(h3_data[h3_data['delay'] == delay]) > 0 else 0 for delay in delays]
        
        x = np.arange(len(delays))
        width = 0.35
        
        bars1 = ax1.bar(x - width/2, h2_throughputs, width, label='HTTP/2', color='blue', alpha=0.7)
        bars2 = ax1.bar(x + width/2, h3_throughputs, width, label='HTTP/3', color='orange', alpha=0.7)
        
        ax1.set_xlabel('é…å»¶ (ms)')
        ax1.set_ylabel('ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (req/s)')
        ax1.set_title('ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¯”è¼ƒ')
        ax1.set_xticks(x)
        ax1.set_xticklabels([f'{d}ms' for d in delays])
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¯”è¼ƒï¼ˆçµ¶å¯¾å€¤ï¼‰
        ax2 = axes[0, 1]
        h2_latencies = [h2_data[h2_data['delay'] == delay]['latency'].iloc[0] if len(h2_data[h2_data['delay'] == delay]) > 0 else 0 for delay in delays]
        h3_latencies = [h3_data[h3_data['delay'] == delay]['latency'].iloc[0] if len(h3_data[h3_data['delay'] == delay]) > 0 else 0 for delay in delays]
        
        bars3 = ax2.bar(x - width/2, h2_latencies, width, label='HTTP/2', color='blue', alpha=0.7)
        bars4 = ax2.bar(x + width/2, h3_latencies, width, label='HTTP/3', color='orange', alpha=0.7)
        
        ax2.set_xlabel('é…å»¶ (ms)')
        ax2.set_ylabel('ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms)')
        ax2.set_title('ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¯”è¼ƒ')
        ax2.set_xticks(x)
        ax2.set_xticklabels([f'{d}ms' for d in delays])
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. æ¥ç¶šæ™‚é–“æ¯”è¼ƒï¼ˆçµ¶å¯¾å€¤ï¼‰- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®ä¸€éƒ¨ã¨ã—ã¦è¿‘ä¼¼
        ax3 = axes[0, 2]
        # æ¥ç¶šæ™‚é–“ã¯ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®ä¸€å®šå‰²åˆã¨ã—ã¦è¿‘ä¼¼ï¼ˆå®Ÿéš›ã®æ¸¬å®šå€¤ãŒãªã„ãŸã‚ï¼‰
        connection_time_ratio = 0.3  # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®30%ã‚’æ¥ç¶šæ™‚é–“ã¨ã—ã¦ä»®å®š
        h2_connection_times = [lat * connection_time_ratio for lat in h2_latencies]
        h3_connection_times = [lat * connection_time_ratio for lat in h3_latencies]
        
        bars5 = ax3.bar(x - width/2, h2_connection_times, width, label='HTTP/2', color='blue', alpha=0.7)
        bars6 = ax3.bar(x + width/2, h3_connection_times, width, label='HTTP/3', color='orange', alpha=0.7)
        
        ax3.set_xlabel('é…å»¶ (ms)')
        ax3.set_ylabel('æ¥ç¶šæ™‚é–“ (ms)')
        ax3.set_title('æ¥ç¶šæ™‚é–“æ¯”è¼ƒ')
        ax3.set_xticks(x)
        ax3.set_xticklabels([f'{d}ms' for d in delays])
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå„ªä½æ€§ï¼ˆHTTP/3å„ªä½æ€§ï¼‰
        ax4 = axes[1, 0]
        throughput_advantages = []
        for i, delay in enumerate(delays):
            if h3_throughputs[i] > 0:
                advantage = ((h3_throughputs[i] - h2_throughputs[i]) / h2_throughputs[i]) * 100
                throughput_advantages.append(advantage)
            else:
                throughput_advantages.append(0)
        
        colors = ['green' if adv > 0 else 'red' for adv in throughput_advantages]
        bars7 = ax4.bar(x, throughput_advantages, color=colors, alpha=0.7)
        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        
        ax4.set_xlabel('é…å»¶ (ms)')
        ax4.set_ylabel('HTTP/3å„ªä½æ€§ (%)')
        ax4.set_title('ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå„ªä½æ€§')
        ax4.set_xticks(x)
        ax4.set_xticklabels([f'{d}ms' for d in delays])
        ax4.grid(True, alpha=0.3)
        
        # 5. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å„ªä½æ€§ï¼ˆHTTP/3å„ªä½æ€§ï¼‰
        ax5 = axes[1, 1]
        latency_advantages = []
        for i, delay in enumerate(delays):
            if h2_latencies[i] > 0:
                advantage = ((h2_latencies[i] - h3_latencies[i]) / h2_latencies[i]) * 100
                latency_advantages.append(advantage)
            else:
                latency_advantages.append(0)
        
        colors = ['green' if adv > 0 else 'red' for adv in latency_advantages]
        bars8 = ax5.bar(x, latency_advantages, color=colors, alpha=0.7)
        ax5.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        
        ax5.set_xlabel('é…å»¶ (ms)')
        ax5.set_ylabel('HTTP/3å„ªä½æ€§ (%)')
        ax5.set_title('ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å„ªä½æ€§')
        ax5.set_xticks(x)
        ax5.set_xticklabels([f'{d}ms' for d in delays])
        ax5.grid(True, alpha=0.3)
        
        # 6. æ¥ç¶šæ™‚é–“å„ªä½æ€§ï¼ˆHTTP/3å„ªä½æ€§ï¼‰
        ax6 = axes[1, 2]
        connection_advantages = []
        for i, delay in enumerate(delays):
            if h2_connection_times[i] > 0:
                advantage = ((h2_connection_times[i] - h3_connection_times[i]) / h2_connection_times[i]) * 100
                connection_advantages.append(advantage)
            else:
                connection_advantages.append(0)
        
        colors = ['green' if adv > 0 else 'red' for adv in connection_advantages]
        bars9 = ax6.bar(x, connection_advantages, color=colors, alpha=0.7)
        ax6.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        
        ax6.set_xlabel('é…å»¶ (ms)')
        ax6.set_ylabel('HTTP/3å„ªä½æ€§ (%)')
        ax6.set_title('æ¥ç¶šæ™‚é–“å„ªä½æ€§')
        ax6.set_xticks(x)
        ax6.set_xticklabels([f'{d}ms' for d in delays])
        ax6.grid(True, alpha=0.3)
        
        # æ•°å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for ax, bars, values in [(ax1, [bars1, bars2], [h2_throughputs, h3_throughputs]),
                                (ax2, [bars3, bars4], [h2_latencies, h3_latencies]),
                                (ax3, [bars5, bars6], [h2_connection_times, h3_connection_times])]:
            for bar_group, value_group in zip(bars, values):
                for bar, value in zip(bar_group, value_group):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                           f'{value:.1f}', ha='center', va='bottom', fontsize=8)
        
        for ax, values in [(ax4, throughput_advantages), (ax5, latency_advantages), (ax6, connection_advantages)]:
            for i, value in enumerate(values):
                ax.text(i, value + (1 if value >= 0 else -1), f'{value:.1f}%',
                       ha='center', va='bottom' if value >= 0 else 'top', fontsize=8)
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
        plt.subplots_adjust(left=0.08, right=0.95, top=0.92, bottom=0.08, hspace=0.3, wspace=0.25)
        plt.savefig(self.log_dir / 'ultra_final_boundary_analysis.png', dpi=300, bbox_inches='tight', pad_inches=0.1)
        plt.close()
        
        print(f"è¶…æœ€çµ‚ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {self.log_dir / 'ultra_final_boundary_analysis.png'}")
    
    def generate_ultra_report(self):
        """è¶…æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_file = self.log_dir / 'ultra_final_boundary_report.txt'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("è¶…æœ€çµ‚å¢ƒç•Œå€¤åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("ğŸ¯ åˆ†æç›®çš„\n")
            f.write("-" * 30 + "\n")
            f.write("â€¢ HTTP/2ã¨HTTP/3ã®æ€§èƒ½å¢ƒç•Œå€¤ã‚’ç‰¹å®š\n")
            f.write("â€¢ çµ±è¨ˆçš„æœ‰æ„æ€§ã®é–¾å€¤ã‚’å¤§å¹…ã«ç·©å’Œã—ã¦å¢ƒç•Œå€¤ã‚’æ¤œå‡º\n")
            f.write("â€¢ 5å›æ¸¬å®šã«ã‚ˆã‚‹é«˜ä¿¡é ¼æ€§åˆ†æ\n")
            f.write("â€¢ 2Ïƒå¤–ã‚Œå€¤é™¤å»ã«ã‚ˆã‚‹å®‰å®šåŒ–\n\n")
            
            f.write("ğŸ“Š æ¸¬å®šçµ±è¨ˆ\n")
            f.write("-" * 30 + "\n")
            f.write(f"ç·æ¸¬å®šæ•°: {len(self.results)}\n")
            f.write(f"å¢ƒç•Œå€¤æ•°: {len(self.boundaries)}\n")
            f.write(f"ãƒ†ã‚¹ãƒˆæ¡ä»¶æ•°: {len(set([(r['delay'], r['loss'], r['bandwidth']) for r in self.results]))}\n")
            f.write(f"æ¸¬å®šå›æ•°: 5å›/æ¡ä»¶\n")
            f.write(f"å¤–ã‚Œå€¤é™¤å»: 2Ïƒãƒ«ãƒ¼ãƒ«\n\n")
            
            if self.boundaries:
                f.write("ğŸ” æ¤œå‡ºã•ã‚ŒãŸå¢ƒç•Œå€¤\n")
                f.write("-" * 30 + "\n")
                for i, boundary in enumerate(self.boundaries, 1):
                    f.write(f"{i}. é…å»¶: {boundary['delay']}ms, æå¤±: {boundary['loss']}%\n")
                    f.write(f"   HTTP/2: {boundary['h2_throughput']:.1f} Â± {boundary['h2_std']:.1f} req/s\n")
                    f.write(f"   HTTP/3: {boundary['h3_throughput']:.1f} Â± {boundary['h3_std']:.1f} req/s\n")
                    f.write(f"   æ€§èƒ½å·®: {boundary['diff_pct']:.1f}% (ä¿¡é ¼åº¦: {boundary['confidence_level']*100:.0f}%)\n")
                    f.write(f"   å„ªä½ãƒ—ãƒ­ãƒˆã‚³ãƒ«: {boundary['superior_protocol']}\n\n")
            else:
                f.write("âŒ å¢ƒç•Œå€¤ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\n\n")
            
            f.write("ğŸ“ˆ ä¸»è¦ãªç™ºè¦‹\n")
            f.write("-" * 30 + "\n")
            if self.results:
                # æœ€ã‚‚å¤§ããªæ€§èƒ½å·®ã‚’ç‰¹å®š
                max_diff = 0
                max_diff_condition = None
                
                for result in self.results:
                    if result['protocol'] == 'http2':
                        h2_result = result
                        h3_results = [r for r in self.results if r['protocol'] == 'http3' and 
                                     r['delay'] == result['delay'] and r['loss'] == result['loss']]
                        if h3_results:
                            h3_result = h3_results[0]
                            diff = abs(h2_result['throughput'] - h3_result['throughput']) / h3_result['throughput'] * 100
                            if diff > max_diff:
                                max_diff = diff
                                max_diff_condition = (result['delay'], result['loss'])
                
                if max_diff_condition:
                    f.write(f"â€¢ æœ€å¤§æ€§èƒ½å·®: {max_diff:.1f}% (é…å»¶: {max_diff_condition[0]}ms, æå¤±: {max_diff_condition[1]}%)\n")
                
                # HTTP/3ã®ä¸å®‰å®šæ€§
                h3_std_avg = np.mean([r['throughput_std'] for r in self.results if r['protocol'] == 'http3'])
                h2_std_avg = np.mean([r['throughput_std'] for r in self.results if r['protocol'] == 'http2'])
                f.write(f"â€¢ HTTP/3æ¸¬å®šä¸å®‰å®šæ€§: {h3_std_avg:.1f} req/s (HTTP/2: {h2_std_avg:.1f} req/s)\n")
        
        print(f"è¶…æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_file}")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç”Ÿæˆ
        self.generate_csv_report()
    
    def generate_csv_report(self):
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
        if not self.results:
            return
        
        csv_file = self.log_dir / 'ultra_final_results.csv'
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
        csv_data = []
        for result in self.results:
            csv_data.append({
                'Protocol': result['protocol'].upper(),
                'Delay (ms)': result['delay'],
                'Loss (%)': result['loss'],
                'Bandwidth (Mbps)': result['bandwidth'],
                'Throughput (req/s)': result['throughput'],
                'Throughput Std (req/s)': result['throughput_std'],
                'Latency (ms)': result['latency'],
                'Connection Time (ms)': result.get('connection_time', 0),
                'Measurement Count': result.get('measurement_count', 5)
            })
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        import csv
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            if csv_data:
                writer = csv.DictWriter(f, fieldnames=csv_data[0].keys())
                writer.writeheader()
                writer.writerows(csv_data)
        
        print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {csv_file}")
        
        # æ€§èƒ½æ¯”è¼ƒç”¨ã®CSVã‚‚ç”Ÿæˆ
        self.generate_comparison_csv()
    
    def generate_comparison_csv(self):
        """æ€§èƒ½æ¯”è¼ƒç”¨CSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
        if not self.results:
            return
        
        comparison_file = self.log_dir / 'performance_comparison.csv'
        
        # æ¡ä»¶ã”ã¨ã«HTTP/2ã¨HTTP/3ã®çµæœã‚’æ¯”è¼ƒ
        comparison_data = []
        
        # æ¡ä»¶ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        conditions = set()
        for result in self.results:
            conditions.add((result['delay'], result['loss'], result['bandwidth']))
        
        for delay, loss, bandwidth in conditions:
            h2_results = [r for r in self.results if r['protocol'] == 'http2' and 
                         r['delay'] == delay and r['loss'] == loss and r['bandwidth'] == bandwidth]
            h3_results = [r for r in self.results if r['protocol'] == 'http3' and 
                         r['delay'] == delay and r['loss'] == loss and r['bandwidth'] == bandwidth]
            
            if h2_results and h3_results:
                h2_result = h2_results[0]
                h3_result = h3_results[0]
                
                # æ€§èƒ½å·®ã‚’è¨ˆç®—
                throughput_diff = ((h2_result['throughput'] - h3_result['throughput']) / h3_result['throughput']) * 100
                latency_diff = ((h2_result['latency'] - h3_result['latency']) / h3_result['latency']) * 100
                
                comparison_data.append({
                    'Delay (ms)': delay,
                    'Loss (%)': loss,
                    'Bandwidth (Mbps)': bandwidth,
                    'HTTP/2 Throughput (req/s)': h2_result['throughput'],
                    'HTTP/3 Throughput (req/s)': h3_result['throughput'],
                    'HTTP/2 Latency (ms)': h2_result['latency'],
                    'HTTP/3 Latency (ms)': h3_result['latency'],
                    'HTTP/2 Connection Time (ms)': h2_result.get('connection_time', 0),
                    'HTTP/3 Connection Time (ms)': h3_result.get('connection_time', 0),
                    'Throughput Advantage (%)': throughput_diff,
                    'Latency Advantage (%)': latency_diff,
                    'Connection Advantage (%)': 0,  # æ¥ç¶šæ™‚é–“ã®å·®ã‚‚è¨ˆç®—å¯èƒ½
                    'Superior Protocol': 'HTTP/2' if throughput_diff > 0 else 'HTTP/3'
                })
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        import csv
        with open(comparison_file, 'w', newline='', encoding='utf-8') as f:
            if comparison_data:
                writer = csv.DictWriter(f, fieldnames=comparison_data[0].keys())
                writer.writeheader()
                writer.writerows(comparison_data)
        
        print(f"æ€§èƒ½æ¯”è¼ƒCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {comparison_file}")

    def generate_timestamp_bar_graph(self, csv_file, protocol, delay, loss, bandwidth):
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®æ£’ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        try:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            timestamps = []
            response_times = []
            
            with open(csv_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        parts = line.split('\t')
                        if len(parts) >= 3:
                            timestamp = int(parts[0])
                            response_time = int(parts[2])
                            timestamps.append(timestamp)
                            response_times.append(response_time)
            
            if not timestamps:
                print(f"      è­¦å‘Š: CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“: {csv_file}")
                return None
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç›¸å¯¾æ™‚é–“ï¼ˆç§’ï¼‰ã«å¤‰æ›
            start_time = min(timestamps)
            relative_times = [(t - start_time) / 1e9 for t in timestamps]  # ãƒŠãƒç§’ã‹ã‚‰ç§’ã«å¤‰æ›
            
            # ã‚°ãƒ©ãƒ•ç”Ÿæˆ
            plt.figure(figsize=(15, 8))
            
            # ãƒ¡ã‚¤ãƒ³ã®æ£’ã‚°ãƒ©ãƒ•ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼‰
            plt.subplot(2, 1, 1)
            bars = plt.bar(range(len(relative_times)), response_times, 
                          color='skyblue', alpha=0.7, width=0.8)
            plt.title(f'{protocol.upper()} ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æ - ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“\n'
                     f'æ¡ä»¶: é…å»¶ {delay}ms, æå¤± {loss}%, å¸¯åŸŸ {bandwidth}Mbps', 
                     fontsize=14, fontweight='bold')
            plt.xlabel('ãƒªã‚¯ã‚¨ã‚¹ãƒˆé †åº', fontsize=12)
            plt.ylabel('ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ (Î¼s)', fontsize=12)
            plt.grid(True, alpha=0.3)
            
            # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
            avg_response = np.mean(response_times)
            std_response = np.std(response_times)
            plt.text(0.02, 0.98, f'å¹³å‡: {avg_response:.1f}Î¼s\næ¨™æº–åå·®: {std_response:.1f}Î¼s', 
                    transform=plt.gca().transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®åˆ†å¸ƒ
            plt.subplot(2, 1, 2)
            plt.plot(relative_times, range(len(relative_times)), 'o-', 
                    color='red', alpha=0.7, linewidth=1, markersize=3)
            plt.title('ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†å¸ƒ', fontsize=12, fontweight='bold')
            plt.xlabel('ç›¸å¯¾æ™‚é–“ (ç§’)', fontsize=12)
            plt.ylabel('ãƒªã‚¯ã‚¨ã‚¹ãƒˆé †åº', fontsize=12)
            plt.grid(True, alpha=0.3)
            
            # æ™‚é–“é–“éš”ã®çµ±è¨ˆ
            if len(relative_times) > 1:
                intervals = np.diff(relative_times)
                avg_interval = np.mean(intervals)
                plt.text(0.02, 0.98, f'å¹³å‡é–“éš”: {avg_interval:.3f} ç§’', 
                        transform=plt.gca().transAxes, verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            plt.tight_layout()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            graph_file = csv_file.replace('.csv', '_timestamp_analysis.png')
            plt.savefig(graph_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"      ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ä¿å­˜: {graph_file}")
            return graph_file
            
        except Exception as e:
            print(f"      ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ç”Ÿæˆå¤±æ•—: {e}")
            return None
    
    def generate_detailed_timestamp_analysis(self, csv_file, protocol, delay, loss, bandwidth):
        """è©³ç´°ãªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆï¼ˆå€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰"""
        try:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            timestamps = []
            response_times = []
            request_sizes = []
            
            with open(csv_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        parts = line.split('\t')
                        if len(parts) >= 3:
                            timestamp = int(parts[0])
                            request_size = int(parts[1])
                            response_time = int(parts[2])
                            timestamps.append(timestamp)
                            request_sizes.append(request_size)
                            response_times.append(response_time)
            
            if not timestamps:
                return None
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç›¸å¯¾æ™‚é–“ã«å¤‰æ›
            start_time = min(timestamps)
            relative_times = [(t - start_time) / 1e9 for t in timestamps]
            
            # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
            base_name = csv_file.replace('.csv', '')
            graph_files = []
            
            # 1. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®æ£’ã‚°ãƒ©ãƒ•
            plt.figure(figsize=(12, 8))
            plt.bar(range(len(response_times)), response_times, 
                   color='skyblue', alpha=0.7)
            plt.title(f'{protocol.upper()} ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“åˆ†å¸ƒ\n'
                     f'æ¡ä»¶: é…å»¶ {delay}ms, æå¤± {loss}%, å¸¯åŸŸ {bandwidth}Mbps', 
                     fontweight='bold', fontsize=14)
            plt.xlabel('ãƒªã‚¯ã‚¨ã‚¹ãƒˆé †åº', fontsize=12)
            plt.ylabel('ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ (Î¼s)', fontsize=12)
            plt.grid(True, alpha=0.3)
            
            # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
            avg_response = np.mean(response_times)
            std_response = np.std(response_times)
            plt.text(0.02, 0.98, f'å¹³å‡: {avg_response:.1f}Î¼s\næ¨™æº–åå·®: {std_response:.1f}Î¼s', 
                    transform=plt.gca().transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            response_time_file = f"{base_name}_response_time_distribution.png"
            plt.savefig(response_time_file, dpi=300, bbox_inches='tight')
            plt.close()
            graph_files.append(response_time_file)
            print(f"      ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“åˆ†å¸ƒã‚°ãƒ©ãƒ•ä¿å­˜: {response_time_file}")
            
            # 2. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
            plt.figure(figsize=(12, 8))
            plt.hist(response_times, bins=20, color='lightgreen', alpha=0.7, edgecolor='black')
            plt.title(f'{protocol.upper()} ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ \n'
                     f'æ¡ä»¶: é…å»¶ {delay}ms, æå¤± {loss}%, å¸¯åŸŸ {bandwidth}Mbps', 
                     fontweight='bold', fontsize=14)
            plt.xlabel('ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ (Î¼s)', fontsize=12)
            plt.ylabel('é »åº¦', fontsize=12)
            plt.grid(True, alpha=0.3)
            
            histogram_file = f"{base_name}_response_time_histogram.png"
            plt.savefig(histogram_file, dpi=300, bbox_inches='tight')
            plt.close()
            graph_files.append(histogram_file)
            print(f"      ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ä¿å­˜: {histogram_file}")
            
            # 3. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®æ™‚ç³»åˆ—
            plt.figure(figsize=(12, 8))
            plt.plot(relative_times, 'o-', color='red', alpha=0.7, markersize=3)
            plt.title(f'{protocol.upper()} ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ™‚ç³»åˆ—\n'
                     f'æ¡ä»¶: é…å»¶ {delay}ms, æå¤± {loss}%, å¸¯åŸŸ {bandwidth}Mbps', 
                     fontweight='bold', fontsize=14)
            plt.xlabel('ãƒªã‚¯ã‚¨ã‚¹ãƒˆé †åº', fontsize=12)
            plt.ylabel('ç›¸å¯¾æ™‚é–“ (ç§’)', fontsize=12)
            plt.grid(True, alpha=0.3)
            
            timeseries_file = f"{base_name}_timestamp_timeseries.png"
            plt.savefig(timeseries_file, dpi=300, bbox_inches='tight')
            plt.close()
            graph_files.append(timeseries_file)
            print(f"      ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ä¿å­˜: {timeseries_file}")
            
            # 4. æ™‚é–“é–“éš”ã®åˆ†å¸ƒ
            if len(relative_times) > 1:
                intervals = np.diff(relative_times)
                plt.figure(figsize=(12, 8))
                plt.hist(intervals, bins=15, color='orange', alpha=0.7, edgecolor='black')
                plt.title(f'{protocol.upper()} æ™‚é–“é–“éš”åˆ†å¸ƒ\n'
                         f'æ¡ä»¶: é…å»¶ {delay}ms, æå¤± {loss}%, å¸¯åŸŸ {bandwidth}Mbps', 
                         fontweight='bold', fontsize=14)
                plt.xlabel('æ™‚é–“é–“éš” (ç§’)', fontsize=12)
                plt.ylabel('é »åº¦', fontsize=12)
                plt.grid(True, alpha=0.3)
                
                # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
                avg_interval = np.mean(intervals)
                std_interval = np.std(intervals)
                plt.text(0.02, 0.98, f'å¹³å‡é–“éš”: {avg_interval:.3f} ç§’\né–“éš”æ¨™æº–åå·®: {std_interval:.3f} ç§’', 
                        transform=plt.gca().transAxes, verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                
                interval_file = f"{base_name}_time_interval_distribution.png"
                plt.savefig(interval_file, dpi=300, bbox_inches='tight')
                plt.close()
                graph_files.append(interval_file)
                print(f"      æ™‚é–“é–“éš”åˆ†å¸ƒã‚°ãƒ©ãƒ•ä¿å­˜: {interval_file}")
            
            # 5. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®ç´¯ç©åˆ†å¸ƒ
            plt.figure(figsize=(12, 8))
            sorted_response_times = np.sort(response_times)
            cumulative_prob = np.arange(1, len(sorted_response_times) + 1) / len(sorted_response_times)
            plt.plot(sorted_response_times, cumulative_prob, 'b-', linewidth=2)
            plt.title(f'{protocol.upper()} ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ç´¯ç©åˆ†å¸ƒ\n'
                     f'æ¡ä»¶: é…å»¶ {delay}ms, æå¤± {loss}%, å¸¯åŸŸ {bandwidth}Mbps', 
                     fontweight='bold', fontsize=14)
            plt.xlabel('ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ (Î¼s)', fontsize=12)
            plt.ylabel('ç´¯ç©ç¢ºç‡', fontsize=12)
            plt.grid(True, alpha=0.3)
            
            # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ç·šã‚’è¿½åŠ 
            percentiles = [50, 75, 90, 95, 99]
            for p in percentiles:
                value = np.percentile(response_times, p)
                plt.axvline(x=value, color='red', linestyle='--', alpha=0.7)
                plt.text(value, 0.5, f'{p}%', rotation=90, verticalalignment='center')
            
            cumulative_file = f"{base_name}_response_time_cumulative.png"
            plt.savefig(cumulative_file, dpi=300, bbox_inches='tight')
            plt.close()
            graph_files.append(cumulative_file)
            print(f"      ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ç´¯ç©åˆ†å¸ƒã‚°ãƒ©ãƒ•ä¿å­˜: {cumulative_file}")
            
            # 6. çµ±è¨ˆæƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼‰
            stats_text = f"""
{protocol.upper()} ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æçµ±è¨ˆ
æ¡ä»¶: é…å»¶ {delay}ms, æå¤± {loss}%, å¸¯åŸŸ {bandwidth}Mbps

åŸºæœ¬çµ±è¨ˆ:
â€¢ ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {len(response_times)}
â€¢ å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {np.mean(response_times):.1f} Î¼s
â€¢ æ¨™æº–åå·®: {np.std(response_times):.1f} Î¼s
â€¢ æœ€å°: {np.min(response_times)} Î¼s
â€¢ æœ€å¤§: {np.max(response_times)} Î¼s
â€¢ ä¸­å¤®å€¤: {np.median(response_times):.1f} Î¼s
â€¢ 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {np.percentile(response_times, 95):.1f} Î¼s
â€¢ 99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {np.percentile(response_times, 99):.1f} Î¼s

æ™‚é–“é–“éš”çµ±è¨ˆ:
"""
            if len(relative_times) > 1:
                intervals = np.diff(relative_times)
                stats_text += f"""â€¢ å¹³å‡é–“éš”: {np.mean(intervals):.3f} ç§’
â€¢ é–“éš”æ¨™æº–åå·®: {np.std(intervals):.3f} ç§’
â€¢ æœ€å°é–“éš”: {np.min(intervals):.3f} ç§’
â€¢ æœ€å¤§é–“éš”: {np.max(intervals):.3f} ç§’
"""
            
            stats_file = f"{base_name}_timestamp_statistics.txt"
            with open(stats_file, 'w', encoding='utf-8') as f:
                f.write(stats_text)
            graph_files.append(stats_file)
            print(f"      çµ±è¨ˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {stats_file}")
            
            return graph_files
            
        except Exception as e:
            print(f"      è©³ç´°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æç”Ÿæˆå¤±æ•—: {e}")
            return None

    def generate_averaged_csv(self, csv_files, protocol, delay, loss, bandwidth, output_dir):
        """è¤‡æ•°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¹³å‡åŒ–ã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
        try:
            if not csv_files:
                return None
            
            # å…¨ã¦ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            all_data = []
            for csv_file in csv_files:
                try:
                    with open(csv_file, 'r') as f:
                        lines = f.readlines()
                        for line in lines:
                            line = line.strip()
                            if line and not line.startswith('#'):
                                parts = line.split('\t')
                                if len(parts) >= 3:
                                    timestamp = int(parts[0])
                                    request_size = int(parts[1])
                                    response_time = int(parts[2])
                                    all_data.append((timestamp, request_size, response_time))
                except Exception as e:
                    print(f"      CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {csv_file}: {e}")
                    continue
            
            if not all_data:
                print(f"      æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return None
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
            all_data.sort(key=lambda x: x[0])
            
            # å¹³å‡åŒ–ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
            averaged_csv = output_dir / f"{protocol}_{delay}ms_{loss}pct_{bandwidth}mbps_averaged.csv"
            
            with open(averaged_csv, 'w') as f:
                f.write(f"# Protocol: {protocol}\n")
                f.write(f"# Delay: {delay}ms\n")
                f.write(f"# Loss: {loss}%\n")
                f.write(f"# Bandwidth: {bandwidth}Mbps\n")
                f.write(f"# Averaged from {len(csv_files)} CSV files\n")
                f.write(f"# Total requests: {len(all_data)}\n")
                f.write(f"# Timestamp(ns)\tRequestSize(bytes)\tResponseTime(us)\n")
                
                for timestamp, request_size, response_time in all_data:
                    f.write(f"{timestamp}\t{request_size}\t{response_time}\n")
            
            print(f"      å¹³å‡åŒ–CSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: {len(all_data)} ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")
            return str(averaged_csv)
            
        except Exception as e:
            print(f"      å¹³å‡åŒ–CSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå¤±æ•—: {e}")
            return None

def generate_timestamp_graphs_from_csv(csv_file, protocol='http2', delay=0, loss=0, bandwidth=0):
    """æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
    try:
        analyzer = UltraFinalAnalyzer(Path(csv_file).parent)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã‚’æŠ½å‡º
        filename = Path(csv_file).name
        if 'ms' in filename and 'pct' in filename:
            try:
                # ä¾‹: http2_150ms_3pct_0mbps.csv
                parts = filename.replace('.csv', '').split('_')
                if len(parts) >= 3:
                    delay = int(parts[1].replace('ms', ''))
                    loss = int(parts[2].replace('pct', ''))
                    if len(parts) > 3:
                        bandwidth = int(parts[3].replace('mbps', ''))
            except Exception:
                pass
        
        print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ç”Ÿæˆ: {csv_file}")
        print(f"æ¡ä»¶: ãƒ—ãƒ­ãƒˆã‚³ãƒ«={protocol}, é…å»¶={delay}ms, æå¤±={loss}%, å¸¯åŸŸ={bandwidth}Mbps")
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ç”Ÿæˆ
        graph_file = analyzer.generate_timestamp_bar_graph(csv_file, protocol, delay, loss, bandwidth)
        
        # è©³ç´°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æç”Ÿæˆ
        detailed_graph = analyzer.generate_detailed_timestamp_analysis(csv_file, protocol, delay, loss, bandwidth)
        
        if graph_file:
            print(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ä¿å­˜: {graph_file}")
        if detailed_graph:
            print(f"è©³ç´°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ†æä¿å­˜: {detailed_graph}")
        
        return graph_file, detailed_graph
        
    except Exception as e:
        print(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ç”Ÿæˆå¤±æ•—: {e}")
        return None, None

def main():
    parser = argparse.ArgumentParser(description='è¶…æœ€çµ‚å¢ƒç•Œå€¤åˆ†æ')
    parser.add_argument('--log_dir', default='logs/ultra_final_analysis', help='ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--test_conditions', nargs='+', 
                       default=['10:0:0', '50:1:0', '100:2:0', '150:3:0', '200:5:0'],
                       help='ãƒ†ã‚¹ãƒˆæ¡ä»¶ (é…å»¶:æå¤±:å¸¯åŸŸ)')
    parser.add_argument('--csv_file', help='æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ')
    
    args = parser.parse_args()
    
    if args.csv_file:
        # æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ£’ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
        generate_timestamp_graphs_from_csv(args.csv_file)
        return
    
    # é€šå¸¸ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    analyzer = UltraFinalAnalyzer(args.log_dir)
    
    print("è¶…æœ€çµ‚å¢ƒç•Œå€¤åˆ†æé–‹å§‹")
    print(f"ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.log_dir}")
    print(f"ãƒ†ã‚¹ãƒˆæ¡ä»¶: {args.test_conditions}")
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    for condition in args.test_conditions:
        try:
            delay, loss, bandwidth = map(int, condition.split(':'))
            print(f"\næ¡ä»¶: é…å»¶={delay}ms, æå¤±={loss}%, å¸¯åŸŸ={bandwidth}Mbps")
            
            # HTTP/2ãƒ†ã‚¹ãƒˆ
            h2_result = analyzer.run_ultra_reliable_benchmark(delay, loss, bandwidth, 'http2')
            if h2_result:
                analyzer.results.append(h2_result)
            
            # HTTP/3ãƒ†ã‚¹ãƒˆ
            h3_result = analyzer.run_ultra_reliable_benchmark(delay, loss, bandwidth, 'http3')
            if h3_result:
                analyzer.results.append(h3_result)
        
        except ValueError as e:
            print(f"æ¡ä»¶ã®è§£æã‚¨ãƒ©ãƒ¼: {condition} - {e}")
            continue
    
    if analyzer.results:
        print("\nå¢ƒç•Œå€¤æ¤œå‡ºé–‹å§‹")
        analyzer.detect_ultra_boundaries()
        
        print("\nã‚°ãƒ©ãƒ•ç”Ÿæˆé–‹å§‹")
        analyzer.generate_ultra_graphs()
    
        print("\nãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
        analyzer.generate_ultra_report()
    
        print(f"\nåˆ†æå®Œäº†: {args.log_dir}")
    else:
        print("æœ‰åŠ¹ãªçµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    main() 